{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "import os\n",
    "import dateutil\n",
    "import time\n",
    "import re\n",
    "import psycopg2\n",
    "import pandas.io.sql as psql\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical data\n",
    "import calendar\n",
    "\n",
    "def next_weekday(d, weekday):\n",
    "    days_ahead = weekday - d.weekday()\n",
    "    if days_ahead < 0: # Target day already happened this week\n",
    "        days_ahead += 7\n",
    "    return d + datetime.timedelta(days_ahead)\n",
    "\n",
    "def bucketed(df, start_on=\"Sunday\"):\n",
    "    df['day_of_week'] = pd.to_datetime(df['created_at']).dt.day_name()\n",
    "    \n",
    "    df['created_at'] = pd.to_datetime(df['created_at']).dt.date\n",
    "    min_date = df[\"created_at\"].min()\n",
    "    min_date = next_weekday(min_date, list(calendar.day_name).index(start_on))\n",
    "    max_date = df[\"created_at\"].max()\n",
    "    tweet_counts = df.groupby('created_at').agg('count')[\"text\"]\n",
    "    dates = pd.date_range(min_date, max_date, freq='D')\n",
    "    counts = pd.DataFrame({ \"count\": tweet_counts},index=dates).fillna(0)\n",
    "    counts = counts.resample('7D').sum()\n",
    "    return counts.drop(counts.tail(1).index) # drop last row in case its a count over less than the full time bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_BALANCE = 58\n",
    "N_TWEETS = 0\n",
    "\n",
    "BEARER_TOKEN=\"AAAAAAAAAAAAAAAAAAAAAAXT9gAAAAAAoITLBCf%2B2K7BMSqakqcbsHUSLrk%3DLz95o8CkkhjOTthpcyEEg6BdNav0zphRcrEYdeG4GXXV3Qkftk\"\n",
    "\n",
    "## value functions\n",
    "def expected_value(potential_win, chance_win, potential_loss, chance_loss):\n",
    "    return (potential_win * chance_win) - (potential_loss * chance_loss)\n",
    "\n",
    "def allocation(account_balance, expected_value):\n",
    "    pct_alloc = min(( expected_value * 5 ) / 10, .03)\n",
    "    alloc = account_balance * pct_alloc\n",
    "    #risk_coef = 1 - (1 / (proba * 100) )\n",
    "    #risk_adjusted = alloc * risk_coef\n",
    "    #return risk_adjusted\n",
    "    return alloc\n",
    "def allocation(price_per_share, proba):\n",
    "    payoff_odds = (1 / price_per_share) - 1\n",
    "\n",
    "def recommended_shares(account_balance, expected_value, price_per_share):\n",
    "    return allocation(account_balance, expected_value) / price_per_share;\n",
    "\n",
    "def to_proba(buckets, categories=None):\n",
    "    vals = buckets.value_counts()\n",
    "    # [ (range(0,2), \"0-2\"), range(3-5), \"3-5\" ]\n",
    "    #for c in categories:\n",
    "    #    rnge = c[0]\n",
    "    #    id_str = c[1]\n",
    "    #    for r in range:\n",
    "            \n",
    "    s = vals.sum()\n",
    "    return vals/s\n",
    "\n",
    "## portfolio management\n",
    "TAX_RATE = .1\n",
    "def kelly_criterion(outcomes):\n",
    "    # category, price_per_share, proba\n",
    "    er = []\n",
    "    betas = []\n",
    "    for index, o in outcomes.iterrows():\n",
    "        payoff_odds = (1 / o[\"price_per_share\"]) - 1\n",
    "        beta = 1 / (1 + payoff_odds)\n",
    "        dividend_rate = 1 - TAX_RATE\n",
    "        expected_revenue_rate = (dividend_rate / beta) * o[\"proba\"]\n",
    "        er.append(expected_revenue_rate)\n",
    "        betas.append(beta)\n",
    "        \n",
    "    outcomes[\"expected_revenue_rate\"] = er\n",
    "    outcomes[\"beta\"] = betas\n",
    "    outcomes = outcomes.sort_values(\"expected_revenue_rate\", ascending=False)\n",
    "    \n",
    "    reserve_rate = 1\n",
    "    optimal_set = pd.DataFrame()\n",
    "    for index, o in outcomes.iterrows():\n",
    "        if o[\"expected_revenue_rate\"] > reserve_rate:\n",
    "            optimal_set = optimal_set.append(o)\n",
    "            reserve_rate = (1 - optimal_set[\"proba\"].sum()) / (1 - (optimal_set[\"beta\"] / dividend_rate).sum())\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    pct_alloc = [] \n",
    "    for index, o in optimal_set.iterrows():\n",
    "        pct = (o[\"expected_revenue_rate\"] - reserve_rate) / ( dividend_rate / o[\"beta\"] )\n",
    "        pct_alloc.append(pct)\n",
    "    optimal_set[\"pct_alloc\"] = pct_alloc\n",
    "    return optimal_set\n",
    "\n",
    "def shares_bought(c, yes_or_no, positions):\n",
    "    bought = 0\n",
    "    if c in positions and yes_or_no in positions[c]:\n",
    "        for pos in positions[c][yes_or_no]:\n",
    "            bought += pos[1]\n",
    "    return bought\n",
    "\n",
    "def recommendation_buy(contract, yes_or_no, account_balance, expected_value, price_per_share, positions):\n",
    "    shares = recommended_shares(account_balance, expected_value, price_per_share) - shares_bought(contract, yes_or_no, positions)\n",
    "    shares = int(round(shares))\n",
    "    if shares > 0:\n",
    "        print(\"BUY {yn} shares for contract {n}: {shares} shares @{price} (EV: {ev}, TOTAL: {t})\".format(n=contract,shares=shares, price=price_per_share, ev=expected_value, yn=yes_or_no.upper(), t=shares*price_per_share))\n",
    "\n",
    "def recommendation_sell(contract, yes_or_no, expected_value, price_per_share, n_shares, bought_at):\n",
    "    print(\"SELL {yn} shares for contract {n}_{bought_at}_{n_shares}: ALL shares @{price} (EV: {ev}, TOTAL: {t})\".format(n=contract, price=price_per_share, ev=expected_value, yn=yes_or_no.upper(), t=n_shares*price_per_share, bought_at=bought_at, n_shares=n_shares))\n",
    "    \n",
    "## market evaluation\n",
    "def fetch_market_data(market_id):\n",
    "    url = \"https://www.predictit.org/api/marketdata/markets/{id}\".format(id=market_id)\n",
    "    r = requests.get(url=url)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_twitter_user_timeline(screen_name, max_id=None, since_id=None):\n",
    "    url = \"https://api.twitter.com/1.1/statuses/user_timeline.json\"\n",
    "    headers = { \"Authorization\": \"Bearer {t}\".format(t=BEARER_TOKEN)}\n",
    "    params = {\n",
    "        \"count\": \"200\",\n",
    "        \"trim_user\": \"true\",\n",
    "        \"screen_name\": screen_name\n",
    "    }\n",
    "    if max_id: \n",
    "        params[\"max_id\"] = max_id\n",
    "    if since_id:\n",
    "        params[\"since_id\"] = since_id\n",
    "        \n",
    "    r = requests.get(url=url,headers=headers, params=params)\n",
    "    raw = r.json()\n",
    "    transformed = json.dumps([ { \"id\": tweet[\"id\"], \"created_at\": tweet[\"created_at\"], \"text\": tweet[\"text\"] } for tweet in raw])\n",
    "    return pd.read_json(transformed, orient=\"records\")\n",
    "\n",
    "def get_recent_tweets(screen_name, from_date=None):\n",
    "    df = get_twitter_user_timeline(screen_name)\n",
    "    df[\"created_at\"] = df[\"created_at\"].dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "    if from_date:\n",
    "        df = df[df[\"created_at\"] > from_date]\n",
    "    return df\n",
    "    \n",
    "# the twitter api returns different results for the same request...\n",
    "def _get_twitter_history(screen_name, max_id=None):\n",
    "    get_next = True\n",
    "    df = pd.DataFrame(columns=[\"id\",\"created_at\", \"text\"])\n",
    "    while get_next:\n",
    "        tweets = get_twitter_user_timeline(screen_name, max_id)\n",
    "        print(len(tweets.index))\n",
    "        if len(tweets.index) > 0:\n",
    "            df = tweets if df.empty else pd.concat([df, tweets], axis=0)\n",
    "            last_row = tweets.tail(1).iloc[0]\n",
    "            max_id = last_row[\"id\"] - 1\n",
    "        else:\n",
    "            get_next = False\n",
    "    return df\n",
    "\n",
    "# the twitter api returns different results for the same request...\n",
    "def get_recent_twitter_history(screen_name, since_id):\n",
    "    get_next = True\n",
    "    df = pd.DataFrame(columns=[\"id\",\"created_at\", \"text\"])\n",
    "    while get_next:\n",
    "        tweets = get_twitter_user_timeline(screen_name, since_id=since_id)\n",
    "        print(len(tweets.index))\n",
    "        if len(tweets.index) > 0:\n",
    "            df = tweets if df.empty else pd.concat([df, tweets], axis=0)\n",
    "            most_recent = tweets.head(1).iloc[0] # TODO: prob should do better than this to ensure most recent\n",
    "            since_id = most_recent[\"id\"]\n",
    "        else:\n",
    "            get_next = False\n",
    "    return df\n",
    "\n",
    "# Gets all tweets before the earliest tweet stored locally for the given screen_name.\n",
    "# If we have nothing stored locally for the screen_name, gets all tweets.\n",
    "# NOTE: Twitter returns us most recent (n?) tweets, anything before that requires premium or enterprise account.\n",
    "def get_twitter_history(screen_name, cache=True):\n",
    "    fname = \"data/tweets/{sn}.csv\".format(sn=screen_name)\n",
    "    max_id = None\n",
    "    if cache and os.path.isfile(fname):\n",
    "        df = pd.read_csv(fname)\n",
    "        max_id = int(df.tail(1).iloc[0][\"id\"]) -1\n",
    "    df = _get_twitter_history(screen_name, max_id);\n",
    "    if not os.path.isdir(\"data/tweets\"):\n",
    "        os.mkdir(\"data/tweets\")\n",
    "    if len(df) > 0:\n",
    "        df.to_csv(fname, mode='a', index=False, header=False)\n",
    "\n",
    "# Gets all tweets after the latest tweet stored locally for the given screen_name.\n",
    "# If we have nothing stored locally for the screen_name, gets all tweets.\n",
    "def update_twitter_history(screen_name):\n",
    "    fname = \"data/tweets/{sn}.csv\".format(sn=screen_name)\n",
    "    if not os.path.isfile(fname):\n",
    "        get_twitter_history(screen_name)\n",
    "    else:\n",
    "        # get since_id (most recent entry)\n",
    "        # copy to .bak file\n",
    "        # get most recent tweets starting at since_id\n",
    "        # save df to file\n",
    "        # copy old .bak to file\n",
    "        copyfile(fname, fname + '.bak')\n",
    "        \n",
    "        df = pd.read_csv(fname)\n",
    "        since_id = int(df.head(1).iloc[0][\"id\"])\n",
    "                \n",
    "        recent_tweets = get_recent_twitter_history(screen_name, since_id)\n",
    "        if len(recent_tweets) > 0:\n",
    "            recent_tweets.to_csv(fname, mode='w', index=False)\n",
    "            historical_tweets = pd.read_csv(fname + '.bak')\n",
    "            historical_tweets.to_csv(fname, mode='a', index=False, header=False)\n",
    "\n",
    "def fetch_full_trump_tweet_history(rnge, cache=True):\n",
    "    fname = \"data/tweets/@realDonaldTrump.csv\"\n",
    "    df = None\n",
    "    for year in rnge:\n",
    "        url = None\n",
    "        if year == 2019:\n",
    "            url = \"http://www.trumptwitterarchive.com/data/realdonaldtrump/2019.json\"\n",
    "        else:\n",
    "            url = \"http://d5nxcu7vtzvay.cloudfront.net/data/realdonaldtrump/{y}.json\".format(y=str(year))\n",
    "        _df  = pd.read_json(url)\n",
    "        if df is None:\n",
    "            df = _df\n",
    "        else:\n",
    "            df = pd.concat([df,_df])\n",
    "        time.sleep(1)\n",
    "     \n",
    "    if not os.path.isdir(\"data/tweets\"):\n",
    "        os.mkdir(\"data/tweets\")\n",
    "    if len(df) > 0:\n",
    "        df.to_csv(fname, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\"homieng6@gmail.com\"\n",
    "#\"??\"\n",
    "#\"@homiesaccount\"\n",
    "#\"nY7VUVqcxJ4vmcX\"\n",
    "#\"AAAAAAAAAAAAAAAAAAAAAAXT9gAAAAAAoITLBCf%2B2K7BMSqakqcbsHUSLrk%3DLz95o8CkkhjOTthpcyEEg6BdNav0zphRcrEYdeG4GXXV3Qkft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tweet_distributions_per_day(source_df):\n",
    "    df = pd.DataFrame(columns=[\"proba\",\"day\"])\n",
    "    df.index.name = \"n_tweets\"\n",
    "    for x in range(0,7,1):\n",
    "        weekday = calendar.day_name[x]\n",
    "        b = bucketed(source_df, start_on=weekday)\n",
    "        proba = b['count']/b['count'].sum()\n",
    "        _df = pd.DataFrame({ \"proba\": proba.values, \"day\": x }, index=proba.index)\n",
    "        df = pd.concat([df, _df])\n",
    "        df[\"n_tweets\"] = df.index\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for key, _grp in df.groupby(['n_tweets']):\n",
    "        grp = _grp.sort_values(by=\"day\", ascending=False)\n",
    "        ax = grp.plot(ax=ax, kind='line', x=\"day\", y='proba', label=str(grp[\"n_tweets\"].iloc[0]))\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "#_df = pd.read_csv('./data/fake_news_tweets.csv')\n",
    "#plot_tweet_distributions_per_day(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_twitter_market_research(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # number of tweets per week\n",
    "    b=bucketed(df)\n",
    "    b.plot(title=\"Tweets per Week\")\n",
    "    plt.show()\n",
    "    \n",
    "    # distribution of tweets per week\n",
    "    vals = b[\"count\"].value_counts()\n",
    "    bins = vals.size\n",
    "    b[\"count\"].plot(kind=\"hist\",bins=bins, title=\"Tweets per Week Distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    # freq of tweets per day\n",
    "    df['day_of_week'] = pd.to_datetime(df[\"created_at\"]).dt.tz_localize('UTC').dt.tz_convert('US/Eastern').dt.day_name()\n",
    "    \n",
    "    weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df['day_of_week'].value_counts().reindex(weekdays).plot(kind='bar', title=\"Tweets per Calendar Day\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_range_str(range):\n",
    "    return str(range.start) + \"-\" + str(range.stop-1)\n",
    "\n",
    "def append_count(series, count, category_range):\n",
    "    return series.append(pd.Series([ count ], index=[ to_range_str(category_range) ]))\n",
    "\n",
    "# takes dataframe with tweet counts bucketed per n days\n",
    "# returns a data frame that returns counts for a category based, excluding coun\n",
    "# this answers: what is the probability that we end in a category, given that we have already seen curr_n values\n",
    "def count_adjusted(df, categories, curr_n):\n",
    "    grouped = pd.Series()\n",
    "    for rnge in categories:\n",
    "        adjusted_range = range(max(rnge.start-curr_n, 0), max(rnge.stop-curr_n, 0 ))\n",
    "        count = df[df[\"count\"].between(adjusted_range.start, adjusted_range.stop-1)].shape[0]\n",
    "        grouped = append_count(grouped, count, rnge)\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_start_date(end_date_str):\n",
    "    end_date = parser.parse(end_date_str)\n",
    "    return end_date - datetime.timedelta(days=7)\n",
    "\n",
    "def time_boundaries(market_data, tz):\n",
    "    contracts = market_data[\"contracts\"]\n",
    "    end_date_str = contracts[0][\"dateEnd\"]\n",
    "    end_date = parser.parse(end_date_str)\n",
    "    start_date = end_date - datetime.timedelta(days=7)\n",
    "    return ( tz.localize(start_date), tz.localize(end_date) )\n",
    "    \n",
    "def eval_live_twitter_market(market, path, matching_tweets=None, show_market_research=False, dry_run=True):\n",
    "    if show_market_research:\n",
    "        show_twitter_market_research(path)\n",
    "        \n",
    "    market_data = fetch_market_data(market[\"id\"])\n",
    "    timezone = pytz.timezone(\"US/Eastern\")\n",
    "    utc_now = pytz.utc.localize(datetime.datetime.utcnow())\n",
    "    ts = utc_now.astimezone(pytz.timezone(\"US/Eastern\"))\n",
    "    start_date, end_date = time_boundaries(market_data, timezone)\n",
    "    \n",
    "    matching_tweets = get_recent_tweets(market[\"twitter_handle\"], from_date=start_date)\n",
    "    if ( \"filter\" in market.keys() ):\n",
    "        matching_tweets = matching_tweets[matching_tweets[\"text\"].str.contains(market[\"filter\"],case=False)]\n",
    "        #n_matching_tweets = len(matching_tweets[matching_tweets[\"text\"].str.contains(\"fake news|fakenews\",case=False)])\n",
    "    n_matching_tweets = len(matching_tweets)\n",
    "    eval_twitter_market(market, path, data=market_data, ts=ts, n_matching_tweets=n_matching_tweets, dry_run=dry_run)\n",
    "\n",
    "def eval_twitter_market(market, path, data=None, ts=None, n_matching_tweets=None, show_market_research=False, dry_run=True):\n",
    "    if show_market_research:\n",
    "        show_twitter_market_research(path)\n",
    "        \n",
    "    if data is None:\n",
    "        data = fetch_market_data(market[\"id\"])\n",
    "    \n",
    "    data[\"timezone\"] = pytz.timezone(\"US/Eastern\") # timeStamp field in market data seems to be in US/Eastern \n",
    "    contracts = data[\"contracts\"]\n",
    "    for c in contracts:\n",
    "        c_id = str(c[\"id\"])\n",
    "        annotations = market[\"contract_map\"][c_id]\n",
    "        c[\"range\"] = annotations[\"range\"]\n",
    "        c[\"category\"] = to_range_str(c[\"range\"])\n",
    "    print(data[\"shortName\"])\n",
    "    \n",
    "    start_date, end_date = time_boundaries(data, data[\"timezone\"])\n",
    "    n_days = days_left(end_date, ts) # days left too complicated, can just do end_date - ts\n",
    "    print(\"Days left:\", n_days)\n",
    "    print(\"Matching tweets:\", n_matching_tweets)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    df['day_of_week'] = pd.to_datetime(df[\"created_at\"]).dt.tz_localize('UTC').dt.tz_convert('US/Eastern').dt.day_name()\n",
    "    weekdays = calendar.day_name\n",
    "    circular_weekdays = np.tile(weekdays, 2)\n",
    "    idx = np.where(circular_weekdays == ts.strftime(\"%A\"))[0][0]\n",
    "    weekdays_left = circular_weekdays[idx:idx+n_days]\n",
    "    \n",
    "    df = df[df[\"day_of_week\"].isin(weekdays_left)]   \n",
    "    b=bucketed(df, start_on=weekdays[idx])\n",
    "    c=count_adjusted(b, [c[\"range\"] for c in contracts], n_matching_tweets )\n",
    "    proba = c/c.sum()\n",
    "    print(\"Category probabilities:\")\n",
    "    pprint.pprint(proba)\n",
    "    \n",
    "    category_stats = pd.DataFrame({ \"price_per_share\": [], \"proba\": [] })\n",
    "    for c in contracts:\n",
    "        s = pd.Series({ \"price_per_share\": c[\"bestBuyYesCost\"], \"proba\": proba[c[\"category\"]] })\n",
    "        s.name = c[\"category\"]\n",
    "        category_stats = category_stats.append(s)\n",
    "    alloc = kelly_criterion(category_stats)\n",
    "    print(alloc)\n",
    "    place_orders(market[\"id\"], contracts, alloc, ACCOUNT_BALANCE * .1, dry_run=dry_run)\n",
    "    \n",
    "    #outcomes(positions, [c[\"category\"] for c in contracts])\n",
    "\n",
    "def days_left(end_date, ts):\n",
    "    start_date = end_date - datetime.timedelta(days=7)\n",
    "    delta = ts - start_date\n",
    "    days_left = ((7*24) - (delta.total_seconds()/3600))/24\n",
    "    return max(round(days_left),1)\n",
    "                    \n",
    "def outcomes(positions, categories):\n",
    "    for c in categories:\n",
    "        total = 0\n",
    "        for pp in positions:\n",
    "            if pp == c:\n",
    "                if \"yes\" in positions[pp]:\n",
    "                    for x in positions[pp][\"yes\"]:\n",
    "                        total += (1 - x[0])*x[1]\n",
    "                if \"no\" in positions[pp]:\n",
    "                    for x in positions[pp][\"no\"]:\n",
    "                        total -= x[0]*x[1]\n",
    "            else:\n",
    "                if \"yes\" in positions[pp]:\n",
    "                    for x in positions[pp][\"yes\"]:\n",
    "                        total -= x[0]*x[1]\n",
    "                if \"no\" in positions[pp]:\n",
    "                    for x in positions[pp][\"no\"]:\n",
    "                        total += (1-x[0])*x[1]\n",
    "        print(c, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale EV by risk for final quantity recommendations (to reduce volatility)\n",
    "# take expected tweets for day of week into account given some people dont tweet much on weekends\n",
    "# graph of tweet density per time per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = [\n",
    "    { \n",
    "        \"id\": 5457, \n",
    "        \"twitter_handle\": \"@vp\", \n",
    "        \"contract_map\": {\n",
    "            \"15263\": { \"range\": range(0, 20) }, \n",
    "            \"15267\": { \"range\": range(20, 25) }, \n",
    "            \"15266\": { \"range\": range(25, 30) }, \n",
    "            \"15268\": { \"range\": range(30, 35) },\n",
    "            \"15264\": { \"range\": range(35, 40) },\n",
    "            \"15269\": { \"range\": range(40, 45) },\n",
    "            \"15265\": { \"range\": range(45, 100) }\n",
    "        },\n",
    "        \"positions\":{ \n",
    "        }\n",
    "    },\n",
    "    { \n",
    "        \"id\": 5407, \n",
    "        \"twitter_handle\": \"@whitehouse\", \n",
    "        \"contract_map\": {\n",
    "            \"14983\": { \"range\": range(0, 80) }, \n",
    "            \"14985\": { \"range\": range(80, 85) }, \n",
    "            \"14984\": { \"range\": range(85, 90) },\n",
    "            \"14986\": { \"range\": range(90, 95) },\n",
    "            \"14987\": { \"range\": range(95, 100) },\n",
    "            \"14988\": { \"range\": range(100, 105) }, \n",
    "            \"14989\": { \"range\": range(105, 300) }\n",
    "        },\n",
    "        #\"contract_map\": [ (\"14983\", range(0, 80)), (\"14985\",range(80, 85)), (\"14984\", range(85, 90)), (\"14986\", range(90, 95)), (\"14987\",range(95, 100)), (\"14988\", range(100, 105)), (\"14989\",range(105, 300))],\n",
    "        \"positions\": { \n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5404, \n",
    "        \"twitter_handle\": \"@realDonaldTrump\",\n",
    "        \"contract_map\": {\n",
    "            \"14968\": { \"range\": range(0, 60) }, \n",
    "            \"14963\": { \"range\": range(60, 65) }, \n",
    "            \"14967\": { \"range\": range(65, 70) },\n",
    "            \"14965\": { \"range\": range(70, 75) },\n",
    "            \"14964\": { \"range\": range(75, 80) },\n",
    "            \"14966\": { \"range\": range(80, 85) }, \n",
    "            \"14962\": { \"range\": range(85, 200) }\n",
    "        },\n",
    "        #\"contract_map\": [ (\"14968\", range(0, 60)), (\"14963\",range(60, 65)), (\"14967\", range(65, 70)), (\"14965\", range(70, 75)), (\"14964\",range(75, 80)), (\"14966\", range(80, 85)), (\"14962\",range(85, 200))],\n",
    "        \"positions\": {\n",
    "            #\"0-59\": {\n",
    "            #    \"yes\": [(.12, 11)]\n",
    "            #},\n",
    "            #\"60-64\": {\n",
    "            #    \"yes\": [(.11, 12)]\n",
    "            #},\n",
    "            #\"70-74\": {\n",
    "            #    \"yes\": [(.04, 20)]\n",
    "            #},\n",
    "            #\"80-84\": {\n",
    "            #    \"no\": [(.69, 4)]\n",
    "            #},\n",
    "            #\"85-199\": {\n",
    "            #    \"no\": [(.74, 4),(.64,1),(.54,1), (.34, 3)]\n",
    "            #}\n",
    "        }\n",
    "    },\n",
    "    { \n",
    "        \"id\": 5458, \n",
    "        \"twitter_handle\": \"@potus\", \n",
    "        \"contract_map\": {\n",
    "            \"15270\": { \"range\": range(0, 45) }, \n",
    "            \"15274\": { \"range\": range(45, 50) },\n",
    "            \"15275\": { \"range\": range(50, 55) },\n",
    "            \"15271\": { \"range\": range(55, 60) }, \n",
    "            \"15272\": { \"range\": range(60, 65) }, \n",
    "            \"15273\": { \"range\": range(65, 69) },\n",
    "            \"15276\": { \"range\": range(70, 200) }\n",
    "        },\n",
    "        \"positions\": {\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def eval_live_markets(show_market_research=False, dry_run=True):\n",
    "    for market in markets:\n",
    "        eval_live_twitter_market(\n",
    "            market,\n",
    "            \"data/tweets/{handle}.csv\".format(handle=market[\"twitter_handle\"]),\n",
    "            show_market_research=show_market_research,\n",
    "            dry_run=dry_run\n",
    "        )\n",
    "        print(\"----------------------------------------\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct = .005\n",
    "# 2 pct = .01\n",
    "# 4 pct = .02\n",
    "# 8 pct = .03\n",
    "def alloc(expected_value, proba): \n",
    "    pct_alloc = min( expected_value / 2, .03)\n",
    "    risk_adjusted = pct_alloc# * ??\n",
    "    return risk_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_orders(market_id, contracts, optimal_set, account_balance, dry_run=True):\n",
    "    for contract in contracts:\n",
    "        category = contract[\"category\"]\n",
    "        price_per_share = contract[\"bestBuyYesCost\"]\n",
    "        current_quantity = current_alloc(market_id, contract[\"id\"])\n",
    "        \n",
    "        if category not in optimal_set.index:\n",
    "            # TODO: this doesnt take into consideration sell price, which in this market is usually less than buy price\n",
    "            # could sell at best buy price...\n",
    "            if current_quantity > 0:\n",
    "                place_order({\n",
    "                    \"action\": \"sell\",\n",
    "                    \"category\": category,\n",
    "                    \"type\": \"yes\", \n",
    "                    \"price_per_share\": price_per_share,\n",
    "                    \"quantity\": current_quantity,\n",
    "                    #\"ev\": \"unknown\",\n",
    "                    \"market_id\": market_id,\n",
    "                    \"contract_id\": contract[\"id\"]\n",
    "                }, dry_run=dry_run)\n",
    "        else:\n",
    "            row = optimal_set.loc[category,:]\n",
    "            optimal_alloc = (row[\"pct_alloc\"] * account_balance)\n",
    "            optimal_quantity = round( abs(optimal_alloc / price_per_share) )\n",
    "            quantity = optimal_quantity - current_quantity\n",
    "\n",
    "            if quantity > 0:\n",
    "                place_order({\n",
    "                    \"action\": \"buy\", \n",
    "                    \"category\": category,\n",
    "                    \"type\": \"yes\", \n",
    "                    \"price_per_share\": price_per_share,\n",
    "                    \"quantity\": quantity,\n",
    "                    #\"ev\": row[\"proba\"] - row[\"price_per_share\"],\n",
    "                    \"market_id\": market_id,\n",
    "                    \"contract_id\": contract[\"id\"]\n",
    "                }, dry_run=dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn = psycopg2.connect(database=\"predictit\", host=\"localhost\", port=\"5432\")\n",
    "#conn = create_engine(\"postgresql+psycopg2://@localhost:5432/predictit\"\n",
    "db_string = \"postgresql+psycopg2://@localhost:5432/predictit\"\n",
    "def current_alloc(market_id, contract_id):\n",
    "    contract_orders = psql.read_sql(\"SELECT * from orders WHERE market_id = \\'{m_id}\\' AND contract_id = \\'{c_id}\\'\".format(m_id=market_id, c_id=contract_id), db_string)\n",
    "\n",
    "    quantity = 0\n",
    "    for i,o in contract_orders.iterrows():\n",
    "        multiplier = -1 if o[\"action\"] == \"sell\" else 1\n",
    "        quantity += o[\"quantity\"] * multiplier\n",
    "    return quantity\n",
    "\n",
    "def place_order(order, verbose=True, dry_run=True):\n",
    "    df = pd.Series(order).to_frame().transpose()\n",
    "    print(df)\n",
    "    if not dry_run:\n",
    "        df.to_sql('orders', con=db_string, if_exists='append', index=False)\n",
    "    if verbose:\n",
    "        print(order)\n",
    "        \n",
    "def record_timepoint(market_id=5458):\n",
    "    data = fetch_market_data(market_id)\n",
    "    twitter_handle = re.match(r\".*@(\\w{1,15})\",data[\"shortName\"]).group(0).split(' ')[-1]\n",
    "    df = pd.Series({ \"timestamp\": data[\"timeStamp\"], \"market_id\": data[\"id\"], \"handle\": twitter_handle, \"data\": json.dumps(data) }).to_frame().transpose()\n",
    "    df.to_sql('market_data', con=db_string, if_exists='append', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta</th>\n",
       "      <th>expected_revenue_rate</th>\n",
       "      <th>price_per_share</th>\n",
       "      <th>proba</th>\n",
       "      <th>pct_alloc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beta  expected_revenue_rate  price_per_share  proba  pct_alloc\n",
       "1   0.1                   2.70              0.1    0.3        0.4\n",
       "2   0.2                   1.35              0.2    0.3        0.5\n",
       "3   0.4                   1.35              0.4    0.6        1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bought_at: .10\n",
    "# EV: .70\n",
    "# price: .75\n",
    "# SELL (not in optimal set)\n",
    "\n",
    "# bought_at: .10\n",
    "# EV: .70\n",
    "# price: .50\n",
    "# BUY (but would have more shares than recommended)\n",
    "\n",
    "# bought_at: .10 \n",
    "# EV: .70\n",
    "# price: .05\n",
    "# BUY (difference over current alloc)\n",
    "\n",
    "# bought_at: .90\n",
    "# EV: .70\n",
    "# price: .75\n",
    "# SELL (not in optimal set)\n",
    "\n",
    "# bought_at: .90\n",
    "# EV: .70\n",
    "# price: .50\n",
    "# BUY (likely allocation is less than what you have, in which case you sell)\n",
    "\n",
    "# bought_at: .90\n",
    "# EV: -.10\n",
    "# price: .50\n",
    "#\n",
    "\n",
    "market = { \n",
    "    \"id\": 5411, \n",
    "    \"twitter_handle\": \"@potus\", \n",
    "    \"contract_map\": [ (\"15008\", range(0, 35)), (\"15010\",range(35, 40)), (\"15011\", range(40, 45)), (\"15012\", range(45, 50)), (\"15013\",range(50, 55)), (\"15009\", range(55, 60)), (\"15014\",range(60, 200))]\n",
    "}\n",
    "df = pd.DataFrame({ \"price_per_share\": [.2, .51, .40, .01, .10], \"proba\": [.30, .10, .60, .02, .7] }, index=[\"0-59\", \"60-64\", \"65-69\", \"70-71\", \"test\"])\n",
    "df = pd.DataFrame({ \"price_per_share\": [.10, .2, .40], \"proba\": [.3, .30, .60] }, index=[\"1\", \"2\", \"3\"])\n",
    "alloc = kelly_criterion(df)\n",
    "alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@vp tweets noon 4/26 - noon 5/3?\n",
      "Days left: 1\n",
      "Matching tweets: 66\n",
      "Category probabilities:\n",
      "45-99    1.0\n",
      "0-19     0.0\n",
      "35-39    0.0\n",
      "25-29    0.0\n",
      "20-24    0.0\n",
      "30-34    0.0\n",
      "40-44    0.0\n",
      "dtype: float64\n",
      "Empty DataFrame\n",
      "Columns: [pct_alloc]\n",
      "Index: []\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "@whitehouse tweets 4/4 - 4/11?\n",
      "Days left: 1\n",
      "Matching tweets: 200\n",
      "Category probabilities:\n",
      "105-299    1.0\n",
      "0-79       0.0\n",
      "85-89      0.0\n",
      "80-84      0.0\n",
      "90-94      0.0\n",
      "95-99      0.0\n",
      "100-104    0.0\n",
      "dtype: float64\n",
      "Empty DataFrame\n",
      "Columns: [pct_alloc]\n",
      "Index: []\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "@realDonaldTrump tweets 4/3 - 4/10?\n",
      "Days left: 1\n",
      "Matching tweets: 198\n",
      "Category probabilities:\n",
      "85-199    1.0\n",
      "60-64     0.0\n",
      "75-79     0.0\n",
      "70-74     0.0\n",
      "80-84     0.0\n",
      "65-69     0.0\n",
      "0-59      0.0\n",
      "dtype: float64\n",
      "Empty DataFrame\n",
      "Columns: [pct_alloc]\n",
      "Index: []\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "@potus tweets noon 4/26 - noon 5/3?\n",
      "Days left: 1\n",
      "Matching tweets: 80\n",
      "Category probabilities:\n",
      "55-59     0.0\n",
      "0-44      0.0\n",
      "60-64     0.0\n",
      "65-68     0.0\n",
      "45-49     0.0\n",
      "50-54     0.0\n",
      "70-199    1.0\n",
      "dtype: float64\n",
      "Empty DataFrame\n",
      "Columns: [pct_alloc]\n",
      "Index: []\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_live_markets(show_market_research=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_market(market):\n",
    "    historical_data = psql.read_sql(\"SELECT * from market_data WHERE handle = \\'{handle}\\'\".format(handle=market[\"twitter_handle\"]), db_string)\n",
    "    path = \"data/tweets/{handle}.csv\".format(handle=market[\"twitter_handle\"])\n",
    "    for i, data_point in historical_data.iterrows():\n",
    "        timezone = pytz.timezone(\"US/Eastern\")\n",
    "        ts = timezone.localize( parser.parse(data_point[\"timestamp\"]) )\n",
    "        market_data = json.loads(data_point[\"data\"])\n",
    "        start_date, end_date = time_boundaries(market_data, timezone)\n",
    "        \n",
    "        df = get_historical_twitter_data(path)\n",
    "        \n",
    "        if df[df[\"created_at\"] > ts].empty():\n",
    "            print('Updating twitter history for {handle}...'.format(handle=market[\"twitter_handle\"]))\n",
    "            update_twitter_history(market[\"twitter_handle\"])\n",
    "            df = get_historical_twitter_data(path)\n",
    "\n",
    "        print(ts, start_date, end_date)\n",
    "        return df[(df[\"created_at\"] >= start_date) & (df[\"created_at\"] <= ts)]\n",
    "        eval_twitter_market(market, path, data=market_data, ts=ts, show_market_research=False)\n",
    "\n",
    "def get_historical_twitter_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "    df[\"created_at\"] = df[\"created_at\"].dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "    return df\n",
    "\n",
    "def simulate_markets():\n",
    "    for m in markets:\n",
    "        simulate_market(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Unknown string format:', 'created_at')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike\u001b[0;34m(arg, box, format, name, tz)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime_to_datetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-0370e3b005b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulate_market\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-7fd62ce6ec90>\u001b[0m in \u001b[0;36msimulate_market\u001b[0;34m(market)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created_at\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created_at\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created_at\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"created_at\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UTC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'US/Eastern'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike\u001b[0;34m(arg, box, format, name, tz)\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike\u001b[0;34m(arg, box, format, name, tz)\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0mdayfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0myearfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                     \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m                 )\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown string format:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Unknown string format:', 'created_at')"
     ]
    }
   ],
   "source": [
    "simulate_market(markets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          Friday\n",
       "1          Friday\n",
       "2          Friday\n",
       "3          Friday\n",
       "4          Friday\n",
       "5          Friday\n",
       "6          Friday\n",
       "7        Thursday\n",
       "8        Thursday\n",
       "9        Thursday\n",
       "10       Thursday\n",
       "11       Thursday\n",
       "12      Wednesday\n",
       "13      Wednesday\n",
       "14      Wednesday\n",
       "15      Wednesday\n",
       "16      Wednesday\n",
       "17      Wednesday\n",
       "18      Wednesday\n",
       "19      Wednesday\n",
       "20      Wednesday\n",
       "21      Wednesday\n",
       "22      Wednesday\n",
       "23        Tuesday\n",
       "24        Tuesday\n",
       "25        Tuesday\n",
       "26        Tuesday\n",
       "27        Tuesday\n",
       "28        Tuesday\n",
       "29        Tuesday\n",
       "          ...    \n",
       "3181     Thursday\n",
       "3182     Thursday\n",
       "3183     Thursday\n",
       "3184     Thursday\n",
       "3185     Thursday\n",
       "3186     Thursday\n",
       "3187     Thursday\n",
       "3188     Thursday\n",
       "3189     Thursday\n",
       "3190     Thursday\n",
       "3191    Wednesday\n",
       "3192    Wednesday\n",
       "3193    Wednesday\n",
       "3194    Wednesday\n",
       "3195    Wednesday\n",
       "3196    Wednesday\n",
       "3197    Wednesday\n",
       "3198    Wednesday\n",
       "3199    Wednesday\n",
       "3200    Wednesday\n",
       "3201    Wednesday\n",
       "3202      Tuesday\n",
       "3203      Tuesday\n",
       "3204      Tuesday\n",
       "3205      Tuesday\n",
       "3206      Tuesday\n",
       "3207       Sunday\n",
       "3208       Sunday\n",
       "3209     Saturday\n",
       "3210     Saturday\n",
       "Name: created_at, Length: 3211, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(df[\"created_at\"]).dt.tz_localize('UTC').dt.tz_convert('US/Eastern').dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch_market_data(5478)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = datetime.datetime(2019,5,6,12,0,0)\n",
    "d2 = datetime.datetime(2019,5,4,23,0,0)\n",
    "days_left(d1,d2)\n",
    "#start_date = end_date - datetime.timedelta(days=7)\n",
    "#    delta = ts - start_date\n",
    "#    days_left = ((7*24) - (delta.total_seconds()/3600))/24\n",
    "#    return max(round(days_left),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(1, 46800)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 - d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2019-05-05 18:04:27\n",
       "1       2019-05-04 18:29:24\n",
       "2                created_at\n",
       "3       2019-05-04 17:08:23\n",
       "4       2019-05-04 02:25:38\n",
       "5       2019-05-04 02:21:06\n",
       "6       2019-05-04 00:54:36\n",
       "7       2019-05-03 22:42:49\n",
       "8       2019-05-03 20:48:39\n",
       "9       2019-05-03 20:22:47\n",
       "10      2019-05-03 20:13:07\n",
       "11      2019-05-03 17:53:21\n",
       "12      2019-05-03 17:01:39\n",
       "13      2019-05-03 16:16:10\n",
       "14      2019-05-03 16:01:29\n",
       "15      2019-05-03 15:02:19\n",
       "16      2019-05-03 14:26:54\n",
       "17      2019-05-03 14:24:55\n",
       "18      2019-05-03 14:24:42\n",
       "19      2019-05-03 01:04:37\n",
       "20      2019-05-02 23:53:06\n",
       "21      2019-05-02 19:52:58\n",
       "22      2019-05-02 18:37:47\n",
       "23      2019-05-02 17:32:53\n",
       "24      2019-05-02 17:12:05\n",
       "25      2019-05-02 17:12:01\n",
       "26      2019-05-02 16:21:48\n",
       "27      2019-05-02 14:46:26\n",
       "28      2019-05-02 11:37:24\n",
       "29      2019-05-02 03:20:24\n",
       "               ...         \n",
       "3217    2017-12-29 17:41:30\n",
       "3218    2017-12-28 22:12:37\n",
       "3219    2017-12-27 17:53:13\n",
       "3220    2017-12-27 17:02:36\n",
       "3221    2017-12-26 22:44:06\n",
       "3222    2017-12-25 14:05:32\n",
       "3223    2017-12-24 18:17:47\n",
       "3224    2017-12-23 16:11:59\n",
       "3225    2017-12-23 15:21:22\n",
       "3226    2017-12-23 14:54:06\n",
       "3227    2017-12-22 16:41:11\n",
       "3228    2017-12-22 15:50:58\n",
       "3229    2017-12-22 13:33:54\n",
       "3230    2017-12-22 03:29:36\n",
       "3231    2017-12-22 03:03:28\n",
       "3232    2017-12-22 01:33:43\n",
       "3233    2017-12-21 23:44:06\n",
       "3234    2017-12-21 23:39:18\n",
       "3235    2017-12-21 19:19:41\n",
       "3236    2017-12-21 19:03:01\n",
       "3237    2017-12-21 17:17:03\n",
       "3238    2017-12-21 16:10:05\n",
       "3239    2017-12-21 14:46:17\n",
       "3240    2017-12-21 14:05:47\n",
       "3241    2017-12-21 13:33:20\n",
       "3242    2017-12-21 03:05:35\n",
       "3243    2017-12-21 00:20:29\n",
       "3244    2017-12-20 22:43:49\n",
       "3245    2017-12-20 22:16:10\n",
       "3246    2017-12-20 21:44:32\n",
       "Name: created_at, Length: 3247, dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/tweets/@vp.csv')\n",
    "pd.to_datetime(df[\"created_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "update_twitter_history('@vp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
