{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pytz\n",
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "import os\n",
    "import dateutil\n",
    "import time\n",
    "import re\n",
    "import psycopg2\n",
    "import pandas.io.sql as psql\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical data\n",
    "import calendar\n",
    "\n",
    "def next_weekday(d, weekday):\n",
    "    days_ahead = weekday - d.weekday()\n",
    "    if days_ahead < 0: # Target day already happened this week\n",
    "        days_ahead += 7\n",
    "    return d + datetime.timedelta(days_ahead)\n",
    "\n",
    "def bucketed(df, start_on=\"Sunday\"):    \n",
    "    df['created_at'] = pd.to_datetime(df['created_at']).dt.date\n",
    "    min_date = df[\"created_at\"].min()\n",
    "    min_date = next_weekday(min_date, list(calendar.day_name).index(start_on))\n",
    "    max_date = df[\"created_at\"].max()\n",
    "    tweet_counts = df.groupby('created_at').agg('count')[\"text\"]\n",
    "    dates = pd.date_range(min_date, max_date, freq='D')\n",
    "    counts = pd.DataFrame({ \"count\": tweet_counts},index=dates).fillna(0)\n",
    "    counts = counts.resample('7D').sum()\n",
    "    return counts.drop(counts.tail(1).index) # drop last row in case its a count over less than the full time bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCOUNT_BALANCE = 58\n",
    "N_TWEETS = 0\n",
    "\n",
    "BEARER_TOKEN=\"AAAAAAAAAAAAAAAAAAAAAAXT9gAAAAAAoITLBCf%2B2K7BMSqakqcbsHUSLrk%3DLz95o8CkkhjOTthpcyEEg6BdNav0zphRcrEYdeG4GXXV3Qkftk\"\n",
    "\n",
    "## value functions\n",
    "def expected_value(potential_win, chance_win, potential_loss, chance_loss):\n",
    "    return (potential_win * chance_win) - (potential_loss * chance_loss)\n",
    "\n",
    "def allocation(account_balance, expected_value):\n",
    "    pct_alloc = min(( expected_value * 5 ) / 10, .03)\n",
    "    alloc = account_balance * pct_alloc\n",
    "    #risk_coef = 1 - (1 / (proba * 100) )\n",
    "    #risk_adjusted = alloc * risk_coef\n",
    "    #return risk_adjusted\n",
    "    return alloc\n",
    "def allocation(price_per_share, proba):\n",
    "    payoff_odds = (1 / price_per_share) - 1\n",
    "\n",
    "def recommended_shares(account_balance, expected_value, price_per_share):\n",
    "    return allocation(account_balance, expected_value) / price_per_share;\n",
    "\n",
    "def to_proba(buckets, categories=None):\n",
    "    vals = buckets.value_counts()\n",
    "    # [ (range(0,2), \"0-2\"), range(3-5), \"3-5\" ]\n",
    "    #for c in categories:\n",
    "    #    rnge = c[0]\n",
    "    #    id_str = c[1]\n",
    "    #    for r in range:\n",
    "            \n",
    "    s = vals.sum()\n",
    "    return vals/s\n",
    "\n",
    "## portfolio management\n",
    "TAX_RATE = .1\n",
    "def kelly_criterion(outcomes):\n",
    "    # category, price_per_share, proba\n",
    "    er = []\n",
    "    betas = []\n",
    "    for index, o in outcomes.iterrows():\n",
    "        payoff_odds = (1 / o[\"price_per_share\"]) - 1\n",
    "        beta = 1 / (1 + payoff_odds)\n",
    "        dividend_rate = 1 - TAX_RATE\n",
    "        expected_revenue_rate = (dividend_rate / beta) * o[\"proba\"]\n",
    "        er.append(expected_revenue_rate)\n",
    "        betas.append(beta)\n",
    "        \n",
    "    outcomes[\"expected_revenue_rate\"] = er\n",
    "    outcomes[\"beta\"] = betas\n",
    "    outcomes = outcomes.sort_values(\"expected_revenue_rate\", ascending=False)\n",
    "    \n",
    "    reserve_rate = 1\n",
    "    optimal_set = pd.DataFrame()\n",
    "    for index, o in outcomes.iterrows():\n",
    "        if o[\"expected_revenue_rate\"] > reserve_rate:\n",
    "            optimal_set = optimal_set.append(o)\n",
    "            reserve_rate = (1 - optimal_set[\"proba\"].sum()) / (1 - (optimal_set[\"beta\"] / dividend_rate).sum())\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    pct_alloc = [] \n",
    "    for index, o in optimal_set.iterrows():\n",
    "        pct = (o[\"expected_revenue_rate\"] - reserve_rate) / ( dividend_rate / o[\"beta\"] )\n",
    "        pct_alloc.append(pct)\n",
    "    optimal_set[\"pct_alloc\"] = pct_alloc\n",
    "    return optimal_set\n",
    "\n",
    "def shares_bought(c, yes_or_no, positions):\n",
    "    bought = 0\n",
    "    if c in positions and yes_or_no in positions[c]:\n",
    "        for pos in positions[c][yes_or_no]:\n",
    "            bought += pos[1]\n",
    "    return bought\n",
    "\n",
    "def recommendation_buy(contract, yes_or_no, account_balance, expected_value, price_per_share, positions):\n",
    "    shares = recommended_shares(account_balance, expected_value, price_per_share) - shares_bought(contract, yes_or_no, positions)\n",
    "    shares = int(round(shares))\n",
    "    if shares > 0:\n",
    "        print(\"BUY {yn} shares for contract {n}: {shares} shares @{price} (EV: {ev}, TOTAL: {t})\".format(n=contract,shares=shares, price=price_per_share, ev=expected_value, yn=yes_or_no.upper(), t=shares*price_per_share))\n",
    "\n",
    "def recommendation_sell(contract, yes_or_no, expected_value, price_per_share, n_shares, bought_at):\n",
    "    print(\"SELL {yn} shares for contract {n}_{bought_at}_{n_shares}: ALL shares @{price} (EV: {ev}, TOTAL: {t})\".format(n=contract, price=price_per_share, ev=expected_value, yn=yes_or_no.upper(), t=n_shares*price_per_share, bought_at=bought_at, n_shares=n_shares))\n",
    "    \n",
    "## market evaluation\n",
    "def fetch_market_data(market_id):\n",
    "    url = \"https://www.predictit.org/api/marketdata/markets/{id}\".format(id=market_id)\n",
    "    r = requests.get(url=url)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_twitter_user_timeline(screen_name, max_id=None, since_id=None):\n",
    "    url = \"https://api.twitter.com/1.1/statuses/user_timeline.json\"\n",
    "    headers = { \"Authorization\": \"Bearer {t}\".format(t=BEARER_TOKEN)}\n",
    "    params = {\n",
    "        \"count\": \"200\",\n",
    "        \"trim_user\": \"true\",\n",
    "        \"screen_name\": screen_name\n",
    "    }\n",
    "    if max_id: \n",
    "        params[\"max_id\"] = max_id\n",
    "    if since_id:\n",
    "        params[\"since_id\"] = since_id\n",
    "        \n",
    "    r = requests.get(url=url,headers=headers, params=params)\n",
    "    raw = r.json()\n",
    "    transformed = json.dumps([ { \"id\": tweet[\"id\"], \"created_at\": tweet[\"created_at\"], \"text\": tweet[\"text\"] } for tweet in raw])\n",
    "    return pd.read_json(transformed, orient=\"records\")\n",
    "\n",
    "def get_recent_tweets(screen_name, from_date=None):\n",
    "    df = get_twitter_user_timeline(screen_name)\n",
    "    df[\"created_at\"] = df[\"created_at\"].dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "    if from_date:\n",
    "        df = df[df[\"created_at\"] > from_date]\n",
    "    return df\n",
    "    \n",
    "# the twitter api returns different results for the same request...\n",
    "def _get_twitter_history(screen_name, max_id=None):\n",
    "    get_next = True\n",
    "    df = pd.DataFrame(columns=[\"id\",\"created_at\", \"text\"])\n",
    "    while get_next:\n",
    "        tweets = get_twitter_user_timeline(screen_name, max_id)\n",
    "        print(len(tweets.index))\n",
    "        if len(tweets.index) > 0:\n",
    "            df = tweets if df.empty else pd.concat([df, tweets], axis=0)\n",
    "            last_row = tweets.tail(1).iloc[0]\n",
    "            max_id = last_row[\"id\"] - 1\n",
    "        else:\n",
    "            get_next = False\n",
    "    return df\n",
    "\n",
    "# the twitter api returns different results for the same request...\n",
    "def get_recent_twitter_history(screen_name, since_id):\n",
    "    get_next = True\n",
    "    df = pd.DataFrame(columns=[\"id\",\"created_at\", \"text\"])\n",
    "    while get_next:\n",
    "        tweets = get_twitter_user_timeline(screen_name, since_id=since_id)\n",
    "        print(len(tweets.index))\n",
    "        if len(tweets.index) > 0:\n",
    "            df = tweets if df.empty else pd.concat([df, tweets], axis=0)\n",
    "            most_recent = tweets.head(1).iloc[0] # TODO: prob should do better than this to ensure most recent\n",
    "            since_id = most_recent[\"id\"]\n",
    "        else:\n",
    "            get_next = False\n",
    "    return df\n",
    "\n",
    "# Gets all tweets before the earliest tweet stored locally for the given screen_name.\n",
    "# If we have nothing stored locally for the screen_name, gets all tweets.\n",
    "# NOTE: Twitter returns us most recent (n?) tweets, anything before that requires premium or enterprise account.\n",
    "def get_twitter_history(screen_name, cache=True):\n",
    "    fname = \"data/tweets/{sn}.csv\".format(sn=screen_name)\n",
    "    max_id = None\n",
    "    if cache and os.path.isfile(fname):\n",
    "        df = pd.read_csv(fname)\n",
    "        max_id = int(df.tail(1).iloc[0][\"id\"]) -1\n",
    "    df = _get_twitter_history(screen_name, max_id);\n",
    "    if not os.path.isdir(\"data/tweets\"):\n",
    "        os.mkdir(\"data/tweets\")\n",
    "    if len(df) > 0:\n",
    "        df.to_csv(fname, mode='a', index=False, header=False)\n",
    "\n",
    "# Gets all tweets after the latest tweet stored locally for the given screen_name.\n",
    "# If we have nothing stored locally for the screen_name, gets all tweets.\n",
    "def update_twitter_history(screen_name):\n",
    "    fname = \"data/tweets/{sn}.csv\".format(sn=screen_name)\n",
    "    if not os.path.isfile(fname):\n",
    "        get_twitter_history(screen_name)\n",
    "    else:\n",
    "        # get since_id (most recent entry)\n",
    "        # copy to .bak file\n",
    "        # get most recent tweets starting at since_id\n",
    "        # save df to file\n",
    "        # copy old .bak to file\n",
    "        copyfile(fname, fname + '.bak')\n",
    "        \n",
    "        df = pd.read_csv(fname)\n",
    "        since_id = int(df.head(1).iloc[0][\"id\"])\n",
    "                \n",
    "        recent_tweets = get_recent_twitter_history(screen_name, since_id)\n",
    "        if len(recent_tweets) > 0:\n",
    "            recent_tweets.to_csv(fname, mode='w', index=False)\n",
    "            historical_tweets = pd.read_csv(fname + '.bak')\n",
    "            historical_tweets.to_csv(fname, mode='a', index=False, header=False)\n",
    "\n",
    "def fetch_full_trump_tweet_history(rnge, cache=True):\n",
    "    fname = \"data/tweets/@realDonaldTrump.csv\"\n",
    "    df = None\n",
    "    for year in rnge:\n",
    "        url = None\n",
    "        if year == 2019:\n",
    "            url = \"http://www.trumptwitterarchive.com/data/realdonaldtrump/2019.json\"\n",
    "        else:\n",
    "            url = \"http://d5nxcu7vtzvay.cloudfront.net/data/realdonaldtrump/{y}.json\".format(y=str(year))\n",
    "        _df  = pd.read_json(url)\n",
    "        if df is None:\n",
    "            df = _df\n",
    "        else:\n",
    "            df = pd.concat([df,_df])\n",
    "        time.sleep(1)\n",
    "     \n",
    "    if not os.path.isdir(\"data/tweets\"):\n",
    "        os.mkdir(\"data/tweets\")\n",
    "    if len(df) > 0:\n",
    "        df.to_csv(fname, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\"homieng6@gmail.com\"\n",
    "#\"??\"\n",
    "#\"@homiesaccount\"\n",
    "#\"nY7VUVqcxJ4vmcX\"\n",
    "#\"AAAAAAAAAAAAAAAAAAAAAAXT9gAAAAAAoITLBCf%2B2K7BMSqakqcbsHUSLrk%3DLz95o8CkkhjOTthpcyEEg6BdNav0zphRcrEYdeG4GXXV3Qkft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tweet_distributions_per_day(source_df):\n",
    "    df = pd.DataFrame(columns=[\"proba\",\"day\"])\n",
    "    df.index.name = \"n_tweets\"\n",
    "    for x in range(0,7,1):\n",
    "        weekday = calendar.day_name[x]\n",
    "        b = bucketed(source_df, start_on=weekday)\n",
    "        proba = b['count']/b['count'].sum()\n",
    "        _df = pd.DataFrame({ \"proba\": proba.values, \"day\": x }, index=proba.index)\n",
    "        df = pd.concat([df, _df])\n",
    "        df[\"n_tweets\"] = df.index\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    for key, _grp in df.groupby(['n_tweets']):\n",
    "        grp = _grp.sort_values(by=\"day\", ascending=False)\n",
    "        ax = grp.plot(ax=ax, kind='line', x=\"day\", y='proba', label=str(grp[\"n_tweets\"].iloc[0]))\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "#_df = pd.read_csv('./data/fake_news_tweets.csv')\n",
    "#plot_tweet_distributions_per_day(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_twitter_market_research(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"created_at\"] = pd.to_datetime(df['created_at']).dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "\n",
    "    # number of tweets per week\n",
    "    b=bucketed(df)\n",
    "    b.plot(title=\"Tweets per Week\")\n",
    "    plt.show()\n",
    "    \n",
    "    # distribution of tweets per week\n",
    "    vals = b[\"count\"].value_counts()\n",
    "    bins = vals.size\n",
    "    b[\"count\"].plot(kind=\"hist\",bins=bins, title=\"Tweets per Week Distribution\")\n",
    "    plt.show()\n",
    "    \n",
    "    # freq of tweets per day\n",
    "    df['day_of_week'] = pd.to_datetime(df[\"created_at\"]).dt.day_name()\n",
    "    \n",
    "    weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    df['day_of_week'].value_counts().reindex(weekdays).plot(kind='bar', title=\"Tweets per Calendar Day\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_range_str(range):\n",
    "    return str(range.start) + \"-\" + str(range.stop-1)\n",
    "\n",
    "def append_count(series, count, category_range):\n",
    "    return series.append(pd.Series([ count ], index=[ to_range_str(category_range) ]))\n",
    "\n",
    "# takes dataframe with tweet counts bucketed per n days\n",
    "# returns a data frame that returns counts for a category based, excluding coun\n",
    "# this answers: what is the probability that we end in a category, given that we have already seen curr_n values\n",
    "def count_adjusted(df, categories, curr_n):\n",
    "    grouped = pd.Series()\n",
    "    for rnge in categories:\n",
    "        adjusted_range = range(max(rnge.start-curr_n, 0), max(rnge.stop-curr_n, 0 ))\n",
    "        count = df[df[\"count\"].between(adjusted_range.start, adjusted_range.stop-1)].shape[0]\n",
    "        grouped = append_count(grouped, count, rnge)\n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def market_start_date(end_date_str):\n",
    "    end_date = parser.parse(end_date_str)\n",
    "    return end_date - datetime.timedelta(days=7)\n",
    "\n",
    "def time_boundaries(market_data, tz):\n",
    "    contracts = market_data[\"contracts\"]\n",
    "    end_date_str = contracts[0][\"dateEnd\"]\n",
    "    end_date = parser.parse(end_date_str)\n",
    "    start_date = end_date - datetime.timedelta(days=7)\n",
    "    return ( tz.localize(start_date), tz.localize(end_date) )\n",
    "    \n",
    "def eval_live_twitter_market(market, path, matching_tweets=None, show_market_research=False, dry_run=True):\n",
    "    if show_market_research:\n",
    "        show_twitter_market_research(path)\n",
    "        \n",
    "    market_data = fetch_market_data(market[\"id\"])\n",
    "    timezone = pytz.timezone(\"US/Eastern\")\n",
    "    utc_now = pytz.utc.localize(datetime.datetime.utcnow())\n",
    "    ts = utc_now.astimezone(pytz.timezone(\"US/Eastern\"))\n",
    "    start_date, end_date = time_boundaries(market_data, timezone)\n",
    "    \n",
    "    matching_tweets = get_recent_tweets(market[\"twitter_handle\"], from_date=start_date)\n",
    "    if ( \"filter\" in market.keys() ):\n",
    "        matching_tweets = matching_tweets[matching_tweets[\"text\"].str.contains(market[\"filter\"],case=False)]\n",
    "        #n_matching_tweets = len(matching_tweets[matching_tweets[\"text\"].str.contains(\"fake news|fakenews\",case=False)])\n",
    "    n_matching_tweets = len(matching_tweets)\n",
    "    evaluation = eval_twitter_market(market, path, data=market_data, ts=ts, n_matching_tweets=n_matching_tweets, dry_run=dry_run)\n",
    "    \n",
    "    print(evaluation[\"name\"])\n",
    "    print(\"Days left:\", evaluation[\"days_left\"])\n",
    "    print(\"Number of tweets:\", evaluation[\"n_matching_tweets\"])\n",
    "    print(\"\\nCategory probabilities:\")\n",
    "    print(evaluation[\"category_probabilities\"])\n",
    "    print(\"\\nOptimal allocation:\")\n",
    "    print(evaluation[\"optimal_allocation\"])\n",
    "\n",
    "def eval_twitter_market(market, path, data=None, ts=None, n_matching_tweets=None, show_market_research=False, dry_run=True):\n",
    "    if show_market_research:\n",
    "        show_twitter_market_research(path)\n",
    "        \n",
    "    if data is None:\n",
    "        data = fetch_market_data(market[\"id\"])\n",
    "    \n",
    "    data[\"timezone\"] = pytz.timezone(\"US/Eastern\") # timeStamp field in market data seems to be in US/Eastern \n",
    "    contracts = data[\"contracts\"]\n",
    "    for c in contracts:\n",
    "        c_id = str(c[\"id\"])\n",
    "        annotations = market[\"contract_map\"][c_id]\n",
    "        c[\"range\"] = annotations[\"range\"]\n",
    "        c[\"category\"] = to_range_str(c[\"range\"])\n",
    "    \n",
    "    start_date, end_date = time_boundaries(data, data[\"timezone\"])\n",
    "    n_days = days_left(end_date, ts) # days left too complicated, can just do end_date - ts\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df['day_of_week'] = pd.to_datetime(df[\"created_at\"]).dt.tz_localize('UTC').dt.tz_convert('US/Eastern').dt.day_name()\n",
    "\n",
    "    weekdays = calendar.day_name\n",
    "    circular_weekdays = np.tile(weekdays, 2)\n",
    "    idx = np.where(circular_weekdays == ts.strftime(\"%A\"))[0][0]\n",
    "    weekdays_left = circular_weekdays[idx:idx+n_days]\n",
    "    \n",
    "    df = df[df[\"day_of_week\"].isin(weekdays_left)]   \n",
    "    b=bucketed(df, start_on=weekdays[idx])\n",
    "    c=count_adjusted(b, [c[\"range\"] for c in contracts], n_matching_tweets )\n",
    "    proba = c/c.sum()\n",
    "    \n",
    "    category_stats = pd.DataFrame({ \"price_per_share\": [], \"proba\": [] })\n",
    "    for c in contracts:\n",
    "        s = pd.Series({ \"price_per_share\": c[\"bestBuyYesCost\"], \"proba\": proba[c[\"category\"]] })\n",
    "        s.name = c[\"category\"]\n",
    "        category_stats = category_stats.append(s)\n",
    "    alloc = kelly_criterion(category_stats)\n",
    "    \n",
    "    place_orders(market[\"id\"], contracts, alloc, ACCOUNT_BALANCE * .1, dry_run=dry_run)\n",
    "    \n",
    "    return {\n",
    "        \"name\": data[\"shortName\"],\n",
    "        \"days_left\": n_days,\n",
    "        \"n_matching_tweets\": n_matching_tweets,\n",
    "        \"category_probabilities\": proba,\n",
    "        \"optimal_allocation\": alloc\n",
    "    }\n",
    "    #outcomes(positions, [c[\"category\"] for c in contracts])\n",
    "\n",
    "def days_left(end_date, ts):\n",
    "    start_date = end_date - datetime.timedelta(days=7)\n",
    "    delta = ts - start_date\n",
    "    days_left = ((7*24) - (delta.total_seconds()/3600))/24\n",
    "    return max(round(days_left),1)\n",
    "                    \n",
    "def outcomes(positions, categories):\n",
    "    for c in categories:\n",
    "        total = 0\n",
    "        for pp in positions:\n",
    "            if pp == c:\n",
    "                if \"yes\" in positions[pp]:\n",
    "                    for x in positions[pp][\"yes\"]:\n",
    "                        total += (1 - x[0])*x[1]\n",
    "                if \"no\" in positions[pp]:\n",
    "                    for x in positions[pp][\"no\"]:\n",
    "                        total -= x[0]*x[1]\n",
    "            else:\n",
    "                if \"yes\" in positions[pp]:\n",
    "                    for x in positions[pp][\"yes\"]:\n",
    "                        total -= x[0]*x[1]\n",
    "                if \"no\" in positions[pp]:\n",
    "                    for x in positions[pp][\"no\"]:\n",
    "                        total += (1-x[0])*x[1]\n",
    "        print(c, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale EV by risk for final quantity recommendations (to reduce volatility)\n",
    "# take expected tweets for day of week into account given some people dont tweet much on weekends\n",
    "# graph of tweet density per time per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = [\n",
    "    { \n",
    "        \"id\": 5478, \n",
    "        \"twitter_handle\": \"@vp\", \n",
    "        \"contract_map\": {\n",
    "            \"15348\": { \"range\": range(0, 30) }, \n",
    "            \"15353\": { \"range\": range(30, 35) }, \n",
    "            \"15349\": { \"range\": range(35, 40) }, \n",
    "            \"15354\": { \"range\": range(40, 45) },\n",
    "            \"15352\": { \"range\": range(45, 50) },\n",
    "            \"15351\": { \"range\": range(50, 55) },\n",
    "            \"15350\": { \"range\": range(55, 100) }\n",
    "        },\n",
    "        \"positions\":{ \n",
    "        }\n",
    "    },\n",
    "    { \n",
    "        \"id\": 5407, \n",
    "        \"twitter_handle\": \"@whitehouse\", \n",
    "        \"contract_map\": {\n",
    "            \"14983\": { \"range\": range(0, 80) }, \n",
    "            \"14985\": { \"range\": range(80, 85) }, \n",
    "            \"14984\": { \"range\": range(85, 90) },\n",
    "            \"14986\": { \"range\": range(90, 95) },\n",
    "            \"14987\": { \"range\": range(95, 100) },\n",
    "            \"14988\": { \"range\": range(100, 105) }, \n",
    "            \"14989\": { \"range\": range(105, 300) }\n",
    "        },\n",
    "        #\"contract_map\": [ (\"14983\", range(0, 80)), (\"14985\",range(80, 85)), (\"14984\", range(85, 90)), (\"14986\", range(90, 95)), (\"14987\",range(95, 100)), (\"14988\", range(100, 105)), (\"14989\",range(105, 300))],\n",
    "        \"positions\": { \n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5404, \n",
    "        \"twitter_handle\": \"@realDonaldTrump\",\n",
    "        \"contract_map\": {\n",
    "            \"14968\": { \"range\": range(0, 60) }, \n",
    "            \"14963\": { \"range\": range(60, 65) }, \n",
    "            \"14967\": { \"range\": range(65, 70) },\n",
    "            \"14965\": { \"range\": range(70, 75) },\n",
    "            \"14964\": { \"range\": range(75, 80) },\n",
    "            \"14966\": { \"range\": range(80, 85) }, \n",
    "            \"14962\": { \"range\": range(85, 200) }\n",
    "        },\n",
    "        #\"contract_map\": [ (\"14968\", range(0, 60)), (\"14963\",range(60, 65)), (\"14967\", range(65, 70)), (\"14965\", range(70, 75)), (\"14964\",range(75, 80)), (\"14966\", range(80, 85)), (\"14962\",range(85, 200))],\n",
    "        \"positions\": {\n",
    "            #\"0-59\": {\n",
    "            #    \"yes\": [(.12, 11)]\n",
    "            #},\n",
    "            #\"60-64\": {\n",
    "            #    \"yes\": [(.11, 12)]\n",
    "            #},\n",
    "            #\"70-74\": {\n",
    "            #    \"yes\": [(.04, 20)]\n",
    "            #},\n",
    "            #\"80-84\": {\n",
    "            #    \"no\": [(.69, 4)]\n",
    "            #},\n",
    "            #\"85-199\": {\n",
    "            #    \"no\": [(.74, 4),(.64,1),(.54,1), (.34, 3)]\n",
    "            #}\n",
    "        }\n",
    "    },\n",
    "    { \n",
    "        \"id\": 5458, \n",
    "        \"twitter_handle\": \"@potus\", \n",
    "        \"contract_map\": {\n",
    "            \"15270\": { \"range\": range(0, 45) }, \n",
    "            \"15274\": { \"range\": range(45, 50) },\n",
    "            \"15275\": { \"range\": range(50, 55) },\n",
    "            \"15271\": { \"range\": range(55, 60) }, \n",
    "            \"15272\": { \"range\": range(60, 65) }, \n",
    "            \"15273\": { \"range\": range(65, 69) },\n",
    "            \"15276\": { \"range\": range(70, 200) }\n",
    "        },\n",
    "        \"positions\": {\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def eval_live_markets(show_market_research=False, dry_run=True):\n",
    "    for market in markets:\n",
    "        eval_live_twitter_market(\n",
    "            market,\n",
    "            \"data/tweets/{handle}.csv\".format(handle=market[\"twitter_handle\"]),\n",
    "            show_market_research=show_market_research,\n",
    "            dry_run=dry_run\n",
    "        )\n",
    "        print(\"----------------------------------------\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 pct = .005\n",
    "# 2 pct = .01\n",
    "# 4 pct = .02\n",
    "# 8 pct = .03\n",
    "def alloc(expected_value, proba): \n",
    "    pct_alloc = min( expected_value / 2, .03)\n",
    "    risk_adjusted = pct_alloc# * ??\n",
    "    return risk_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def place_orders(market_id, contracts, optimal_set, account_balance, dry_run=True):\n",
    "    for contract in contracts:\n",
    "        category = contract[\"category\"]\n",
    "        price_per_share = contract[\"bestBuyYesCost\"]\n",
    "        current_quantity = current_alloc(market_id, contract[\"id\"])\n",
    "        \n",
    "        if category not in optimal_set.index:\n",
    "            # TODO: this doesnt take into consideration sell price, which in this market is usually less than buy price\n",
    "            # could sell at best buy price...\n",
    "            if current_quantity > 0:\n",
    "                place_order({\n",
    "                    \"action\": \"sell\",\n",
    "                    \"category\": category,\n",
    "                    \"type\": \"yes\", \n",
    "                    \"price_per_share\": price_per_share,\n",
    "                    \"quantity\": current_quantity,\n",
    "                    #\"ev\": \"unknown\",\n",
    "                    \"market_id\": market_id,\n",
    "                    \"contract_id\": contract[\"id\"]\n",
    "                }, dry_run=dry_run)\n",
    "        else:\n",
    "            row = optimal_set.loc[category,:]\n",
    "            optimal_alloc = (row[\"pct_alloc\"] * account_balance)\n",
    "            optimal_quantity = round( abs(optimal_alloc / price_per_share) )\n",
    "            quantity = optimal_quantity - current_quantity\n",
    "\n",
    "            if quantity > 0:\n",
    "                place_order({\n",
    "                    \"action\": \"buy\", \n",
    "                    \"category\": category,\n",
    "                    \"type\": \"yes\", \n",
    "                    \"price_per_share\": price_per_share,\n",
    "                    \"quantity\": quantity,\n",
    "                    #\"ev\": row[\"proba\"] - row[\"price_per_share\"],\n",
    "                    \"market_id\": market_id,\n",
    "                    \"contract_id\": contract[\"id\"]\n",
    "                }, dry_run=dry_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn = psycopg2.connect(database=\"predictit\", host=\"localhost\", port=\"5432\")\n",
    "#conn = create_engine(\"postgresql+psycopg2://@localhost:5432/predictit\"\n",
    "db_string = \"postgresql+psycopg2://@localhost:5432/predictit\"\n",
    "def current_alloc(market_id, contract_id):\n",
    "    contract_orders = psql.read_sql(\"SELECT * from orders WHERE market_id = \\'{m_id}\\' AND contract_id = \\'{c_id}\\'\".format(m_id=market_id, c_id=contract_id), db_string)\n",
    "\n",
    "    quantity = 0\n",
    "    for i,o in contract_orders.iterrows():\n",
    "        multiplier = -1 if o[\"action\"] == \"sell\" else 1\n",
    "        quantity += o[\"quantity\"] * multiplier\n",
    "    return quantity\n",
    "\n",
    "def place_order(order, verbose=True, dry_run=True):\n",
    "    df = pd.Series(order).to_frame().transpose()\n",
    "    print(df)\n",
    "    if not dry_run:\n",
    "        df.to_sql('orders', con=db_string, if_exists='append', index=False)\n",
    "    if verbose:\n",
    "        print(order)\n",
    "        \n",
    "def record_timepoint(market_id=5458):\n",
    "    new_data = False\n",
    "    last_row = psql.read_sql(\"SELECT * from market_data WHERE market_id = \\'{m_id}\\' ORDER BY timestamp DESC LIMIT 1\".format(m_id=market_id), db_string).iloc[0]\n",
    "    last_ts = last_row[\"timestamp\"]\n",
    "\n",
    "    while new_data is False:\n",
    "        data = fetch_market_data(market_id)\n",
    "        print(data[\"timeStamp\"],last_ts)\n",
    "        if data[\"timeStamp\"] > last_ts:\n",
    "            new_data = True\n",
    "        else:\n",
    "            print(\"Retrying in one second...\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    data = fetch_market_data(market_id)\n",
    "    twitter_handle = re.match(r\".*@(\\w{1,15})\",data[\"shortName\"]).group(0).split(' ')[-1]\n",
    "    df = pd.Series({ \"timestamp\": data[\"timeStamp\"], \"market_id\": data[\"id\"], \"handle\": twitter_handle, \"data\": json.dumps(data) }).to_frame().transpose()\n",
    "    df.to_sql('market_data', con=db_string, if_exists='append', index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'market_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3e8f1cf92f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlast_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT * from orders WHERE market_id = \\'{m_id}\\' AND contract_id = \\'{c_id}\\' ORDER BY timestamp DESC LIMIT 1\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarket_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontract_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlast_ts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_market_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarket_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'market_id' is not defined"
     ]
    }
   ],
   "source": [
    "new_data = False\n",
    "last_row = psql.read_sql(\"SELECT * from orders WHERE market_id = \\'{m_id}\\' AND contract_id = \\'{c_id}\\' ORDER BY timestamp DESC LIMIT 1\".format(m_id=market_id, c_id=contract_id), db_string)[0]\n",
    "last_ts = last_row[\"timestamp\"]\n",
    "while new_data is False:\n",
    "    data = fetch_market_data(market_id)\n",
    "    if data[\"timeStamp\"] > last_ts:\n",
    "        new_data = True\n",
    "    else:\n",
    "        sleep(1)\n",
    "twitter_handle = re.match(r\".*@(\\w{1,15})\",data[\"shortName\"]).group(0).split(' ')[-1]\n",
    "df = pd.Series({ \"timestamp\": data[\"timeStamp\"], \"market_id\": data[\"id\"], \"handle\": twitter_handle, \"data\": json.dumps(data) }).to_frame().transpose()\n",
    "df.to_sql('market_data', con=db_string, if_exists='append', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta</th>\n",
       "      <th>expected_revenue_rate</th>\n",
       "      <th>price_per_share</th>\n",
       "      <th>proba</th>\n",
       "      <th>pct_alloc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beta  expected_revenue_rate  price_per_share  proba  pct_alloc\n",
       "1   0.1                   2.70              0.1    0.3        0.4\n",
       "2   0.2                   1.35              0.2    0.3        0.5\n",
       "3   0.4                   1.35              0.4    0.6        1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bought_at: .10\n",
    "# EV: .70\n",
    "# price: .75\n",
    "# SELL (not in optimal set)\n",
    "\n",
    "# bought_at: .10\n",
    "# EV: .70\n",
    "# price: .50\n",
    "# BUY (but would have more shares than recommended)\n",
    "\n",
    "# bought_at: .10 \n",
    "# EV: .70\n",
    "# price: .05\n",
    "# BUY (difference over current alloc)\n",
    "\n",
    "# bought_at: .90\n",
    "# EV: .70\n",
    "# price: .75\n",
    "# SELL (not in optimal set)\n",
    "\n",
    "# bought_at: .90\n",
    "# EV: .70\n",
    "# price: .50\n",
    "# BUY (likely allocation is less than what you have, in which case you sell)\n",
    "\n",
    "# bought_at: .90\n",
    "# EV: -.10\n",
    "# price: .50\n",
    "#\n",
    "\n",
    "#market = { \n",
    "#    \"id\": 5411, \n",
    "#    \"twitter_handle\": \"@potus\", \n",
    "#    \"contract_map\": [ (\"15008\", range(0, 35)), (\"15010\",range(35, 40)), (\"15011\", range(40, 45)), (\"15012\", range(45, 50)), (\"15013\",range(50, 55)), (\"15009\", range(55, 60)), (\"15014\",range(60, 200))]\n",
    "#}\n",
    "df = pd.DataFrame({ \"price_per_share\": [.2, .51, .40, .01, .10], \"proba\": [.30, .10, .60, .02, .7] }, index=[\"0-59\", \"60-64\", \"65-69\", \"70-71\", \"test\"])\n",
    "df = pd.DataFrame({ \"price_per_share\": [.10, .2, .40], \"proba\": [.3, .30, .60] }, index=[\"1\", \"2\", \"3\"])\n",
    "alloc = kelly_criterion(df)\n",
    "alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    55-99  yes            0.19       12      5478       15350\n",
      "{'action': 'buy', 'category': '55-99', 'type': 'yes', 'price_per_share': 0.19, 'quantity': 12.0, 'market_id': 5478, 'contract_id': 15350}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    30-34  yes            0.01        4      5478       15353\n",
      "{'action': 'buy', 'category': '30-34', 'type': 'yes', 'price_per_share': 0.01, 'quantity': 4.0, 'market_id': 5478, 'contract_id': 15353}\n",
      "@vp tweets noon 5/3 - noon 5/10?\n",
      "Days left: 3\n",
      "Number of tweets: 30\n",
      "\n",
      "Category probabilities:\n",
      "45-49    0.197183\n",
      "50-54    0.154930\n",
      "40-44    0.098592\n",
      "55-99    0.507042\n",
      "35-39    0.028169\n",
      "0-29     0.000000\n",
      "30-34    0.014085\n",
      "dtype: float64\n",
      "\n",
      "Optimal allocation:\n",
      "       beta  expected_revenue_rate  price_per_share     proba  pct_alloc\n",
      "55-99  0.19               2.401779             0.19  0.507042   0.377062\n",
      "30-34  0.01               1.267606             0.01  0.014085   0.007243\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "@whitehouse tweets 4/4 - 4/11?\n",
      "Days left: 1\n",
      "Number of tweets: 200\n",
      "\n",
      "Category probabilities:\n",
      "105-299    1.0\n",
      "0-79       0.0\n",
      "85-89      0.0\n",
      "80-84      0.0\n",
      "90-94      0.0\n",
      "95-99      0.0\n",
      "100-104    0.0\n",
      "dtype: float64\n",
      "\n",
      "Optimal allocation:\n",
      "Empty DataFrame\n",
      "Columns: [pct_alloc]\n",
      "Index: []\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "@realDonaldTrump tweets 4/3 - 4/10?\n",
      "Days left: 1\n",
      "Number of tweets: 197\n",
      "\n",
      "Category probabilities:\n",
      "85-199    1.0\n",
      "60-64     0.0\n",
      "75-79     0.0\n",
      "70-74     0.0\n",
      "80-84     0.0\n",
      "65-69     0.0\n",
      "0-59      0.0\n",
      "dtype: float64\n",
      "\n",
      "Optimal allocation:\n",
      "Empty DataFrame\n",
      "Columns: [pct_alloc]\n",
      "Index: []\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "@potus tweets noon 4/26 - noon 5/3?\n",
      "Days left: 1\n",
      "Number of tweets: 103\n",
      "\n",
      "Category probabilities:\n",
      "55-59     0.0\n",
      "0-44      0.0\n",
      "60-64     0.0\n",
      "65-68     0.0\n",
      "45-49     0.0\n",
      "50-54     0.0\n",
      "70-199    1.0\n",
      "dtype: float64\n",
      "\n",
      "Optimal allocation:\n",
      "Empty DataFrame\n",
      "Columns: [pct_alloc]\n",
      "Index: []\n",
      "----------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_live_markets(show_market_research=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_market(market):\n",
    "    historical_data = psql.read_sql(\"SELECT * from market_data WHERE handle = \\'{handle}\\'\".format(handle=market[\"twitter_handle\"]), db_string)\n",
    "    path = \"data/tweets/{handle}.csv\".format(handle=market[\"twitter_handle\"])\n",
    "    \n",
    "    outpath = \"data/simulations/{handle}/{m_id}\".format(handle=market[\"twitter_handle\"], m_id=market[\"id\"])\n",
    "    if not os.path.isdir(outpath):\n",
    "        os.makedirs(outpath, exist_ok=True)\n",
    "            \n",
    "    update_twitter_history(market[\"twitter_handle\"])\n",
    "    df = get_historical_twitter_data(path)\n",
    "    \n",
    "    price_df = pd.DataFrame()\n",
    "    proba_df = pd.DataFrame()\n",
    "            \n",
    "    for i, data_point in historical_data.iterrows():\n",
    "        timezone = pytz.timezone(\"US/Eastern\")\n",
    "        ts = timezone.localize( parser.parse(data_point[\"timestamp\"]) )\n",
    "        market_data = json.loads(data_point[\"data\"])\n",
    "        start_date, end_date = time_boundaries(market_data, timezone)\n",
    "                \n",
    "        if df[df[\"created_at\"] > ts].empty:\n",
    "            print('WARNING: Historical tweets do not contain tweets greater than the point in time being evaluated.')\n",
    "            print('Likely not an error since we update historical data at the beginning of this function.')\n",
    "        matching_tweets = df[(df[\"created_at\"] >= start_date) & (df[\"created_at\"] <= ts)]\n",
    "        n_matching_tweets = len(matching_tweets)\n",
    "        \n",
    "        evaluation = eval_twitter_market(market, path, data=market_data, ts=ts, n_matching_tweets=n_matching_tweets, show_market_research=False, dry_run=True)\n",
    "        \n",
    "        _prices = {}\n",
    "        for c in market_data[\"contracts\"]:\n",
    "            annotations = market[\"contract_map\"][str(c[\"id\"])]\n",
    "            category = to_range_str(annotations[\"range\"])\n",
    "            _prices[category] = c[\"bestBuyYesCost\"] \n",
    "        prices = pd.Series(_prices)\n",
    "        prices.name = market_data[\"timeStamp\"]\n",
    "        row = prices.to_frame().transpose()\n",
    "        price_df = price_df.append(prices)\n",
    "        \n",
    "        proba = evaluation[\"category_probabilities\"]\n",
    "        proba.name = market_data[\"timeStamp\"]\n",
    "        row = proba.to_frame().transpose()\n",
    "        proba_df = proba_df.append(proba)\n",
    "        \n",
    "        if i > 0 and i % 10 == 0:\n",
    "            append = True if i > 10 else False\n",
    "            write_header = not append\n",
    "            mode = 'a' if append else 'w'\n",
    "            proba_df.to_csv(outpath + \"/proba.csv\", mode=mode, header=write_header)\n",
    "            price_df.to_csv(outpath + \"/prices.csv\", mode=mode, header=write_header)\n",
    "            print(proba_df)\n",
    "            print(price_df)\n",
    "            if i == 20:\n",
    "                return\n",
    "        #proba_df = \n",
    "        \n",
    "        # save category proba at ts /simulations/@vp/market_id\n",
    "        # save category price at ts /simulations/@vp/market_id\n",
    "    \n",
    "        # graph price of each category at point in time\n",
    "        # graph category proba at point in time\n",
    "        # graph tweets vertically\n",
    "def get_historical_twitter_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "    df[\"created_at\"] = df[\"created_at\"].dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "    return df\n",
    "\n",
    "def simulate_markets():\n",
    "    for m in markets:\n",
    "        simulate_market(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.15        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.15, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.15        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.15, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "                                 0-29     30-34     35-39     40-44     45-49  \\\n",
      "2019-05-03T14:07:47.4448846  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:07:47.4448846  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:09:02.6662952  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:10:03.1547018  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:10:03.1547018  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:12:01.6611515  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:13:03.6174877  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:13:03.6174877  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:15:04.2851951  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:15:04.2851951  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:17:02.911794   0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "\n",
      "                               50-54     55-99  \n",
      "2019-05-03T14:07:47.4448846  0.15493  0.309859  \n",
      "2019-05-03T14:07:47.4448846  0.15493  0.309859  \n",
      "2019-05-03T14:09:02.6662952  0.15493  0.309859  \n",
      "2019-05-03T14:10:03.1547018  0.15493  0.309859  \n",
      "2019-05-03T14:10:03.1547018  0.15493  0.309859  \n",
      "2019-05-03T14:12:01.6611515  0.15493  0.309859  \n",
      "2019-05-03T14:13:03.6174877  0.15493  0.309859  \n",
      "2019-05-03T14:13:03.6174877  0.15493  0.309859  \n",
      "2019-05-03T14:15:04.2851951  0.15493  0.309859  \n",
      "2019-05-03T14:15:04.2851951  0.15493  0.309859  \n",
      "2019-05-03T14:17:02.911794   0.15493  0.309859  \n",
      "                             0-29  30-34  35-39  40-44  45-49  50-54  55-99\n",
      "2019-05-03T14:07:47.4448846  0.08   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:07:47.4448846  0.08   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:09:02.6662952  0.07   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:10:03.1547018  0.07   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:10:03.1547018  0.07   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:12:01.6611515  0.06   0.09   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:13:03.6174877  0.06   0.09   0.15   0.14    0.2   0.19   0.34\n",
      "2019-05-03T14:13:03.6174877  0.06   0.09   0.15   0.14    0.2   0.19   0.34\n",
      "2019-05-03T14:15:04.2851951  0.06   0.09   0.14   0.14    0.2   0.19   0.34\n",
      "2019-05-03T14:15:04.2851951  0.06   0.09   0.14   0.14    0.2   0.19   0.34\n",
      "2019-05-03T14:17:02.911794   0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy    35-39  yes            0.14        1      5478       15349\n",
      "{'action': 'buy', 'category': '35-39', 'type': 'yes', 'price_per_share': 0.14, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15349}\n",
      "  action category type price_per_share quantity market_id contract_id\n",
      "0    buy     0-29  yes            0.06        1      5478       15348\n",
      "{'action': 'buy', 'category': '0-29', 'type': 'yes', 'price_per_share': 0.06, 'quantity': 1.0, 'market_id': 5478, 'contract_id': 15348}\n",
      "                                 0-29     30-34     35-39     40-44     45-49  \\\n",
      "2019-05-03T14:07:47.4448846  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:07:47.4448846  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:09:02.6662952  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:10:03.1547018  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:10:03.1547018  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:12:01.6611515  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:13:03.6174877  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:13:03.6174877  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:15:04.2851951  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:15:04.2851951  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:17:02.911794   0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:17:02.911794   0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:19:02.9632415  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:19:02.9632415  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:21:02.92435    0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:21:02.92435    0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:23:03.423901   0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:23:03.423901   0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:24:03.7016186  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:26:02.2599069  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "2019-05-03T14:27:02.9880445  0.070423  0.070423  0.183099  0.098592  0.112676   \n",
      "\n",
      "                               50-54     55-99  \n",
      "2019-05-03T14:07:47.4448846  0.15493  0.309859  \n",
      "2019-05-03T14:07:47.4448846  0.15493  0.309859  \n",
      "2019-05-03T14:09:02.6662952  0.15493  0.309859  \n",
      "2019-05-03T14:10:03.1547018  0.15493  0.309859  \n",
      "2019-05-03T14:10:03.1547018  0.15493  0.309859  \n",
      "2019-05-03T14:12:01.6611515  0.15493  0.309859  \n",
      "2019-05-03T14:13:03.6174877  0.15493  0.309859  \n",
      "2019-05-03T14:13:03.6174877  0.15493  0.309859  \n",
      "2019-05-03T14:15:04.2851951  0.15493  0.309859  \n",
      "2019-05-03T14:15:04.2851951  0.15493  0.309859  \n",
      "2019-05-03T14:17:02.911794   0.15493  0.309859  \n",
      "2019-05-03T14:17:02.911794   0.15493  0.309859  \n",
      "2019-05-03T14:19:02.9632415  0.15493  0.309859  \n",
      "2019-05-03T14:19:02.9632415  0.15493  0.309859  \n",
      "2019-05-03T14:21:02.92435    0.15493  0.309859  \n",
      "2019-05-03T14:21:02.92435    0.15493  0.309859  \n",
      "2019-05-03T14:23:03.423901   0.15493  0.309859  \n",
      "2019-05-03T14:23:03.423901   0.15493  0.309859  \n",
      "2019-05-03T14:24:03.7016186  0.15493  0.309859  \n",
      "2019-05-03T14:26:02.2599069  0.15493  0.309859  \n",
      "2019-05-03T14:27:02.9880445  0.15493  0.309859  \n",
      "                             0-29  30-34  35-39  40-44  45-49  50-54  55-99\n",
      "2019-05-03T14:07:47.4448846  0.08   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:07:47.4448846  0.08   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:09:02.6662952  0.07   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:10:03.1547018  0.07   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:10:03.1547018  0.07   0.10   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:12:01.6611515  0.06   0.09   0.16   0.16    0.2   0.19   0.34\n",
      "2019-05-03T14:13:03.6174877  0.06   0.09   0.15   0.14    0.2   0.19   0.34\n",
      "2019-05-03T14:13:03.6174877  0.06   0.09   0.15   0.14    0.2   0.19   0.34\n",
      "2019-05-03T14:15:04.2851951  0.06   0.09   0.14   0.14    0.2   0.19   0.34\n",
      "2019-05-03T14:15:04.2851951  0.06   0.09   0.14   0.14    0.2   0.19   0.34\n",
      "2019-05-03T14:17:02.911794   0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:17:02.911794   0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:19:02.9632415  0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:19:02.9632415  0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:21:02.92435    0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:21:02.92435    0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:23:03.423901   0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:23:03.423901   0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:24:03.7016186  0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:26:02.2599069  0.06   0.09   0.14   0.14    0.2   0.19   0.33\n",
      "2019-05-03T14:27:02.9880445  0.06   0.09   0.14   0.14    0.2   0.19   0.33\n"
     ]
    }
   ],
   "source": [
    "simulate_market(markets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 5478,\n",
       " 'name': 'How many tweets will @vp post from noon 5/3 - noon 5/10?',\n",
       " 'shortName': '@vp tweets noon 5/3 - noon 5/10?',\n",
       " 'image': 'https://az620379.vo.msecnd.net/images/Markets/a9b732f2-de51-47f2-a306-cb0f14f07f17.png',\n",
       " 'url': 'https://www.predictit.org/markets/detail/5478/How-many-tweets-will-@vp-post-from-noon-5-3-noon-5-10',\n",
       " 'contracts': [{'id': 15350,\n",
       "   'dateEnd': '2019-05-10T12:00:00',\n",
       "   'image': 'https://az620379.vo.msecnd.net/images/Contracts/small_3b509298-d8d7-4938-b84f-70b5de800faa.png',\n",
       "   'name': '55 or more',\n",
       "   'longName': 'Will @vp post 55 or more tweets from noon 5/3 - noon 5/10?',\n",
       "   'shortName': '55+',\n",
       "   'status': 'Open',\n",
       "   'lastTradePrice': 0.4,\n",
       "   'bestBuyYesCost': 0.4,\n",
       "   'bestBuyNoCost': 0.62,\n",
       "   'bestSellYesCost': 0.38,\n",
       "   'bestSellNoCost': 0.6,\n",
       "   'lastClosePrice': 0.39,\n",
       "   'displayOrder': 0},\n",
       "  {'id': 15351,\n",
       "   'dateEnd': '2019-05-10T12:00:00',\n",
       "   'image': 'https://az620379.vo.msecnd.net/images/Contracts/small_c0d04384-c9dc-4d5b-8563-209feb8dec67.png',\n",
       "   'name': '50 - 54',\n",
       "   'longName': 'Will @vp post 50 - 54 tweets from noon 5/3 - noon 5/10?',\n",
       "   'shortName': '50 - 54',\n",
       "   'status': 'Open',\n",
       "   'lastTradePrice': 0.23,\n",
       "   'bestBuyYesCost': 0.25,\n",
       "   'bestBuyNoCost': 0.79,\n",
       "   'bestSellYesCost': 0.21,\n",
       "   'bestSellNoCost': 0.75,\n",
       "   'lastClosePrice': 0.2,\n",
       "   'displayOrder': 0},\n",
       "  {'id': 15352,\n",
       "   'dateEnd': '2019-05-10T12:00:00',\n",
       "   'image': 'https://az620379.vo.msecnd.net/images/Contracts/small_15e92fc1-f19d-4314-801e-7641afc35a13.png',\n",
       "   'name': '45 - 49',\n",
       "   'longName': 'Will @vp post 45 - 49 tweets from noon 5/3 - noon 5/10?',\n",
       "   'shortName': '45 - 49',\n",
       "   'status': 'Open',\n",
       "   'lastTradePrice': 0.2,\n",
       "   'bestBuyYesCost': 0.22,\n",
       "   'bestBuyNoCost': 0.8,\n",
       "   'bestSellYesCost': 0.2,\n",
       "   'bestSellNoCost': 0.78,\n",
       "   'lastClosePrice': 0.19,\n",
       "   'displayOrder': 0},\n",
       "  {'id': 15354,\n",
       "   'dateEnd': '2019-05-10T12:00:00',\n",
       "   'image': 'https://az620379.vo.msecnd.net/images/Contracts/small_7a10f981-7d85-473c-9c4f-ce8e4ef37e14.png',\n",
       "   'name': '40 - 44',\n",
       "   'longName': 'Will @vp post 40 - 44 tweets from noon 5/3 - noon 5/10?',\n",
       "   'shortName': '40 - 44',\n",
       "   'status': 'Open',\n",
       "   'lastTradePrice': 0.2,\n",
       "   'bestBuyYesCost': 0.21,\n",
       "   'bestBuyNoCost': 0.83,\n",
       "   'bestSellYesCost': 0.17,\n",
       "   'bestSellNoCost': 0.79,\n",
       "   'lastClosePrice': 0.17,\n",
       "   'displayOrder': 0},\n",
       "  {'id': 15349,\n",
       "   'dateEnd': '2019-05-10T12:00:00',\n",
       "   'image': 'https://az620379.vo.msecnd.net/images/Contracts/small_725ed7f6-d9f5-490d-aacf-86ffb2d49a9f.png',\n",
       "   'name': '35 - 39',\n",
       "   'longName': 'Will @vp post 35 - 39 tweets from noon 5/3 - noon 5/10?',\n",
       "   'shortName': '35 - 39',\n",
       "   'status': 'Open',\n",
       "   'lastTradePrice': 0.12,\n",
       "   'bestBuyYesCost': 0.11,\n",
       "   'bestBuyNoCost': 0.92,\n",
       "   'bestSellYesCost': 0.08,\n",
       "   'bestSellNoCost': 0.89,\n",
       "   'lastClosePrice': 0.09,\n",
       "   'displayOrder': 0},\n",
       "  {'id': 15353,\n",
       "   'dateEnd': '2019-05-10T12:00:00',\n",
       "   'image': 'https://az620379.vo.msecnd.net/images/Contracts/small_77c65927-e5a3-4585-a38d-6fbfcd902bf7.png',\n",
       "   'name': '30 - 34',\n",
       "   'longName': 'Will @vp post 30 - 34 tweets from noon 5/3 - noon 5/10?',\n",
       "   'shortName': '30 - 34',\n",
       "   'status': 'Open',\n",
       "   'lastTradePrice': 0.06,\n",
       "   'bestBuyYesCost': 0.05,\n",
       "   'bestBuyNoCost': 0.97,\n",
       "   'bestSellYesCost': 0.03,\n",
       "   'bestSellNoCost': 0.95,\n",
       "   'lastClosePrice': 0.04,\n",
       "   'displayOrder': 0},\n",
       "  {'id': 15348,\n",
       "   'dateEnd': '2019-05-10T12:00:00',\n",
       "   'image': 'https://az620379.vo.msecnd.net/images/Contracts/small_8ead3b84-57cf-4e1c-94c0-fe21d9fe1a66.png',\n",
       "   'name': '29 or fewer',\n",
       "   'longName': 'Will @vp post 29 or fewer tweets from noon 5/3 - noon 5/10?',\n",
       "   'shortName': '29-',\n",
       "   'status': 'Open',\n",
       "   'lastTradePrice': 0.01,\n",
       "   'bestBuyYesCost': 0.02,\n",
       "   'bestBuyNoCost': 0.99,\n",
       "   'bestSellYesCost': 0.01,\n",
       "   'bestSellNoCost': 0.98,\n",
       "   'lastClosePrice': 0.02,\n",
       "   'displayOrder': 0}],\n",
       " 'timeStamp': '2019-05-05T19:28:35.9574001',\n",
       " 'status': 'Open'}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_market_data(5478)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = datetime.datetime(2019,5,6,12,0,0)\n",
    "d2 = datetime.datetime(2019,5,4,23,0,0)\n",
    "days_left(d1,d2)\n",
    "#start_date = end_date - datetime.timedelta(days=7)\n",
    "#    delta = ts - start_date\n",
    "#    days_left = ((7*24) - (delta.total_seconds()/3600))/24\n",
    "#    return max(round(days_left),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(1, 46800)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 - d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2019-05-05 23:02:08\n",
       "1      2019-05-05 22:57:55\n",
       "2      2019-05-05 18:04:27\n",
       "3      2019-05-04 18:29:24\n",
       "4      2019-05-04 17:08:23\n",
       "5      2019-05-04 02:25:38\n",
       "6      2019-05-04 02:21:06\n",
       "7      2019-05-04 00:54:36\n",
       "8      2019-05-03 22:42:49\n",
       "9      2019-05-03 20:48:39\n",
       "10     2019-05-03 20:22:47\n",
       "11     2019-05-03 20:13:07\n",
       "12     2019-05-03 17:53:21\n",
       "13     2019-05-03 17:01:39\n",
       "14     2019-05-03 16:16:10\n",
       "15     2019-05-03 16:01:29\n",
       "16     2019-05-03 15:02:19\n",
       "17     2019-05-03 14:26:54\n",
       "18     2019-05-03 14:24:55\n",
       "19     2019-05-03 14:24:42\n",
       "20     2019-05-03 01:04:37\n",
       "21     2019-05-02 23:53:06\n",
       "22     2019-05-02 19:52:58\n",
       "23     2019-05-02 18:37:47\n",
       "24     2019-05-02 17:32:53\n",
       "25     2019-05-02 17:12:05\n",
       "26     2019-05-02 17:12:01\n",
       "27     2019-05-02 16:21:48\n",
       "28     2019-05-02 14:46:26\n",
       "29     2019-05-02 11:37:24\n",
       "               ...        \n",
       "3218   2017-12-29 17:41:30\n",
       "3219   2017-12-28 22:12:37\n",
       "3220   2017-12-27 17:53:13\n",
       "3221   2017-12-27 17:02:36\n",
       "3222   2017-12-26 22:44:06\n",
       "3223   2017-12-25 14:05:32\n",
       "3224   2017-12-24 18:17:47\n",
       "3225   2017-12-23 16:11:59\n",
       "3226   2017-12-23 15:21:22\n",
       "3227   2017-12-23 14:54:06\n",
       "3228   2017-12-22 16:41:11\n",
       "3229   2017-12-22 15:50:58\n",
       "3230   2017-12-22 13:33:54\n",
       "3231   2017-12-22 03:29:36\n",
       "3232   2017-12-22 03:03:28\n",
       "3233   2017-12-22 01:33:43\n",
       "3234   2017-12-21 23:44:06\n",
       "3235   2017-12-21 23:39:18\n",
       "3236   2017-12-21 19:19:41\n",
       "3237   2017-12-21 19:03:01\n",
       "3238   2017-12-21 17:17:03\n",
       "3239   2017-12-21 16:10:05\n",
       "3240   2017-12-21 14:46:17\n",
       "3241   2017-12-21 14:05:47\n",
       "3242   2017-12-21 13:33:20\n",
       "3243   2017-12-21 03:05:35\n",
       "3244   2017-12-21 00:20:29\n",
       "3245   2017-12-20 22:43:49\n",
       "3246   2017-12-20 22:16:10\n",
       "3247   2017-12-20 21:44:32\n",
       "Name: created_at, Length: 3248, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/tweets/@vp.csv')\n",
    "pd.to_datetime(df[\"created_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "update_twitter_history('@vp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>created_at_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-05-06 13:12:32-04:00</td>\n",
       "      <td>1125448270984118272</td>\n",
       "      <td>Coming up at the Satellite 2019 Conference. As...</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2019-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-05-06 12:44:23-04:00</td>\n",
       "      <td>1125441188532838405</td>\n",
       "      <td>What happened at Mt Pleasant Baptist Church, S...</td>\n",
       "      <td>Monday</td>\n",
       "      <td>2019-05-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-05-05 19:02:08-04:00</td>\n",
       "      <td>1125173864642875395</td>\n",
       "      <td>RT @WhiteHouse: 263,000 more jobs, 3.6 percent...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2019-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-05-05 18:57:55-04:00</td>\n",
       "      <td>1125172800828911617</td>\n",
       "      <td>We strongly condemn the attacks in Gaza by Ham...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2019-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-05-05 14:04:27-04:00</td>\n",
       "      <td>1125098948623048705</td>\n",
       "      <td>Wishing all those celebrating a happy Cinco de...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2019-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2019-05-04 14:29:24-04:00</td>\n",
       "      <td>1124742840184201216</td>\n",
       "      <td>As Democrat Governors in NY &amp;amp; VA advocate ...</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2019-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019-05-04 13:08:23-04:00</td>\n",
       "      <td>1124722451986964488</td>\n",
       "      <td>On #InternationalFirefightersDay, we honor tho...</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2019-05-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2019-05-03 22:25:38-04:00</td>\n",
       "      <td>1124500301564784641</td>\n",
       "      <td>For all the success weve seen, @POTUS &amp;amp; I...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2019-05-03 22:21:06-04:00</td>\n",
       "      <td>1124499159044775937</td>\n",
       "      <td>Great to be at @HallwayFeeds in Kentucky w/ @S...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019-05-03 20:54:36-04:00</td>\n",
       "      <td>1124477390313394181</td>\n",
       "      <td>On this 2019 Space Day, we celebrate the extra...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2019-05-03 18:42:49-04:00</td>\n",
       "      <td>1124444225926516737</td>\n",
       "      <td>Great to be back in Kentucky with @SecretaryAc...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2019-05-03 16:48:39-04:00</td>\n",
       "      <td>1124415497448169472</td>\n",
       "      <td>Proud of Law Enforcement, First Responders, &amp;a...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2019-05-03 16:22:47-04:00</td>\n",
       "      <td>1124408984692301825</td>\n",
       "      <td>Honored to be with these congregations. @POTUS...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2019-05-03 16:13:07-04:00</td>\n",
       "      <td>1124406554059472897</td>\n",
       "      <td>Joined the pastors of  Mt Pleasant Baptist Chu...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2019-05-03 13:53:21-04:00</td>\n",
       "      <td>1124371381007863808</td>\n",
       "      <td>Just landed in Louisiana. Thank you for the wa...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2019-05-03 13:01:39-04:00</td>\n",
       "      <td>1124358367420530688</td>\n",
       "      <td>In early 2017, a top economic adviser for the ...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2019-05-03 12:16:10-04:00</td>\n",
       "      <td>1124346922221940736</td>\n",
       "      <td>263,000 jobs in April &amp;amp; the LOWEST unemplo...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2019-05-03 12:01:29-04:00</td>\n",
       "      <td>1124343228919111681</td>\n",
       "      <td>.@POTUS has said from the beginning: if you cu...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2019-05-03 11:02:19-04:00</td>\n",
       "      <td>1124328338179481600</td>\n",
       "      <td>Heading to Opelousas, Louisiana to visit Mount...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2019-05-03 10:26:54-04:00</td>\n",
       "      <td>1124319426482974721</td>\n",
       "      <td>April marks the LOWEST unemployment in nearly ...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2019-05-03 10:24:55-04:00</td>\n",
       "      <td>1124318926165426176</td>\n",
       "      <td>RT @WhiteHouse: Happy Jobs Day. Tired of winni...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2019-05-03 10:24:42-04:00</td>\n",
       "      <td>1124318873703059458</td>\n",
       "      <td>RT @WhiteHouse: It's another historic Jobs Day...</td>\n",
       "      <td>Friday</td>\n",
       "      <td>2019-05-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019-05-02 21:04:37-04:00</td>\n",
       "      <td>1124117525833371649</td>\n",
       "      <td>RT @realDonaldTrump: Proclamation on Days of R...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2019-05-02 19:53:06-04:00</td>\n",
       "      <td>1124099525935869955</td>\n",
       "      <td>In February, @SecondLady &amp;amp; I traveled to P...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2019-05-02 15:52:58-04:00</td>\n",
       "      <td>1124039096391950336</td>\n",
       "      <td>With the strong support of Republicans in the ...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2019-05-02 14:37:47-04:00</td>\n",
       "      <td>1124020173390917632</td>\n",
       "      <td>On behalf of our family, thank you for your pr...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2019-05-02 13:32:53-04:00</td>\n",
       "      <td>1124003843417804801</td>\n",
       "      <td>.@SecondLady &amp;amp; I were honored to celebrate...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2019-05-02 13:12:05-04:00</td>\n",
       "      <td>1123998607089242112</td>\n",
       "      <td>RT @CCSSO: You are an amazing group of indivi...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2019-05-02 13:12:01-04:00</td>\n",
       "      <td>1123998588634316805</td>\n",
       "      <td>RT @SecondLady: Great to be around so much exc...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2019-05-02 12:21:48-04:00</td>\n",
       "      <td>1123985954975383556</td>\n",
       "      <td>We do well to go to the Lord in prayer. Its e...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2019-05-02 10:46:26-04:00</td>\n",
       "      <td>1123961951317581826</td>\n",
       "      <td>RT @WhiteHouse: LIVE at 11 a.m. ET, President ...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2019-05-02 07:37:24-04:00</td>\n",
       "      <td>1123914382973505538</td>\n",
       "      <td>As Venezuelans take to the streets to stand fo...</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>2019-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2019-05-01 23:20:24-04:00</td>\n",
       "      <td>1123789309004402690</td>\n",
       "      <td>RT @WhiteHouse: \"So tonight we praise God for ...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2019-05-01 23:20:14-04:00</td>\n",
       "      <td>1123789266130280448</td>\n",
       "      <td>RT @WhiteHouse: \"During this holy season when ...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2019-05-01 23:20:08-04:00</td>\n",
       "      <td>1123789240410812418</td>\n",
       "      <td>RT @WhiteHouse: \"Tonight we break bread togeth...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2019-05-01 18:39:56-04:00</td>\n",
       "      <td>1123718725394366464</td>\n",
       "      <td>RT @SecondLady: Press Release: Acting Secretar...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2019-05-01 16:07:17-04:00</td>\n",
       "      <td>1123680308350599168</td>\n",
       "      <td>THANK YOU to the brave men and women of @ICEGo...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2019-05-01 16:00:27-04:00</td>\n",
       "      <td>1123678589503856642</td>\n",
       "      <td>Honored to present the Aguila award to the hea...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019-05-01 14:40:52-04:00</td>\n",
       "      <td>1123658560561455107</td>\n",
       "      <td>RT @USCG: Coast Guard spouses will now have ac...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019-05-01 13:21:00-04:00</td>\n",
       "      <td>1123638462203203584</td>\n",
       "      <td>Heading to Baltimore to recognize the great me...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2019-05-01 10:41:15-04:00</td>\n",
       "      <td>1123598262915743744</td>\n",
       "      <td>RT @SecondLady: HAPPENING SOON: Making announc...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2019-05-01 08:29:12-04:00</td>\n",
       "      <td>1123565030518210560</td>\n",
       "      <td>.@ActingSecDef: Its Time to Create an Americ...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2019-05-01 08:24:19-04:00</td>\n",
       "      <td>1123563801146732544</td>\n",
       "      <td>RT @realDonaldTrump: I am overriding the Decom...</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>2019-05-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2019-04-30 17:45:34-04:00</td>\n",
       "      <td>1123342655759503363</td>\n",
       "      <td>RT @realDonaldTrump: ....embargo, together wit...</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2019-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2019-04-30 17:45:29-04:00</td>\n",
       "      <td>1123342633823293440</td>\n",
       "      <td>RT @realDonaldTrump: If Cuban Troops and Milit...</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2019-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2019-04-30 17:30:08-04:00</td>\n",
       "      <td>1123338772660326401</td>\n",
       "      <td>To all of the men &amp;amp; women who serve on the...</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2019-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2019-04-30 16:36:52-04:00</td>\n",
       "      <td>1123325366918164480</td>\n",
       "      <td>Thank you, USS Harry S. Truman! Look forward t...</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2019-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2019-04-30 15:11:18-04:00</td>\n",
       "      <td>1123303834347081728</td>\n",
       "      <td>.@POTUS asked me to deliver a message today  ...</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2019-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2019-04-30 15:07:07-04:00</td>\n",
       "      <td>1123302782243618817</td>\n",
       "      <td>The Truman launched over 2,000 missions throug...</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2019-04-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2019-04-30 15:04:00-04:00</td>\n",
       "      <td>1123301997669748736</td>\n",
       "      <td>For more than 20 years, the USS Truman &amp;amp; i...</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2019-04-30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at                   id  \\\n",
       "0  2019-05-06 13:12:32-04:00  1125448270984118272   \n",
       "1  2019-05-06 12:44:23-04:00  1125441188532838405   \n",
       "2  2019-05-05 19:02:08-04:00  1125173864642875395   \n",
       "3  2019-05-05 18:57:55-04:00  1125172800828911617   \n",
       "4  2019-05-05 14:04:27-04:00  1125098948623048705   \n",
       "5  2019-05-04 14:29:24-04:00  1124742840184201216   \n",
       "6  2019-05-04 13:08:23-04:00  1124722451986964488   \n",
       "7  2019-05-03 22:25:38-04:00  1124500301564784641   \n",
       "8  2019-05-03 22:21:06-04:00  1124499159044775937   \n",
       "9  2019-05-03 20:54:36-04:00  1124477390313394181   \n",
       "10 2019-05-03 18:42:49-04:00  1124444225926516737   \n",
       "11 2019-05-03 16:48:39-04:00  1124415497448169472   \n",
       "12 2019-05-03 16:22:47-04:00  1124408984692301825   \n",
       "13 2019-05-03 16:13:07-04:00  1124406554059472897   \n",
       "14 2019-05-03 13:53:21-04:00  1124371381007863808   \n",
       "15 2019-05-03 13:01:39-04:00  1124358367420530688   \n",
       "16 2019-05-03 12:16:10-04:00  1124346922221940736   \n",
       "17 2019-05-03 12:01:29-04:00  1124343228919111681   \n",
       "18 2019-05-03 11:02:19-04:00  1124328338179481600   \n",
       "19 2019-05-03 10:26:54-04:00  1124319426482974721   \n",
       "20 2019-05-03 10:24:55-04:00  1124318926165426176   \n",
       "21 2019-05-03 10:24:42-04:00  1124318873703059458   \n",
       "22 2019-05-02 21:04:37-04:00  1124117525833371649   \n",
       "23 2019-05-02 19:53:06-04:00  1124099525935869955   \n",
       "24 2019-05-02 15:52:58-04:00  1124039096391950336   \n",
       "25 2019-05-02 14:37:47-04:00  1124020173390917632   \n",
       "26 2019-05-02 13:32:53-04:00  1124003843417804801   \n",
       "27 2019-05-02 13:12:05-04:00  1123998607089242112   \n",
       "28 2019-05-02 13:12:01-04:00  1123998588634316805   \n",
       "29 2019-05-02 12:21:48-04:00  1123985954975383556   \n",
       "30 2019-05-02 10:46:26-04:00  1123961951317581826   \n",
       "31 2019-05-02 07:37:24-04:00  1123914382973505538   \n",
       "32 2019-05-01 23:20:24-04:00  1123789309004402690   \n",
       "33 2019-05-01 23:20:14-04:00  1123789266130280448   \n",
       "34 2019-05-01 23:20:08-04:00  1123789240410812418   \n",
       "35 2019-05-01 18:39:56-04:00  1123718725394366464   \n",
       "36 2019-05-01 16:07:17-04:00  1123680308350599168   \n",
       "37 2019-05-01 16:00:27-04:00  1123678589503856642   \n",
       "38 2019-05-01 14:40:52-04:00  1123658560561455107   \n",
       "39 2019-05-01 13:21:00-04:00  1123638462203203584   \n",
       "40 2019-05-01 10:41:15-04:00  1123598262915743744   \n",
       "41 2019-05-01 08:29:12-04:00  1123565030518210560   \n",
       "42 2019-05-01 08:24:19-04:00  1123563801146732544   \n",
       "43 2019-04-30 17:45:34-04:00  1123342655759503363   \n",
       "44 2019-04-30 17:45:29-04:00  1123342633823293440   \n",
       "45 2019-04-30 17:30:08-04:00  1123338772660326401   \n",
       "46 2019-04-30 16:36:52-04:00  1123325366918164480   \n",
       "47 2019-04-30 15:11:18-04:00  1123303834347081728   \n",
       "48 2019-04-30 15:07:07-04:00  1123302782243618817   \n",
       "49 2019-04-30 15:04:00-04:00  1123301997669748736   \n",
       "\n",
       "                                                 text day_of_week created_at_2  \n",
       "0   Coming up at the Satellite 2019 Conference. As...      Monday   2019-05-06  \n",
       "1   What happened at Mt Pleasant Baptist Church, S...      Monday   2019-05-06  \n",
       "2   RT @WhiteHouse: 263,000 more jobs, 3.6 percent...      Sunday   2019-05-05  \n",
       "3   We strongly condemn the attacks in Gaza by Ham...      Sunday   2019-05-05  \n",
       "4   Wishing all those celebrating a happy Cinco de...      Sunday   2019-05-05  \n",
       "5   As Democrat Governors in NY &amp; VA advocate ...    Saturday   2019-05-04  \n",
       "6   On #InternationalFirefightersDay, we honor tho...    Saturday   2019-05-04  \n",
       "7   For all the success weve seen, @POTUS &amp; I...      Friday   2019-05-03  \n",
       "8   Great to be at @HallwayFeeds in Kentucky w/ @S...      Friday   2019-05-03  \n",
       "9   On this 2019 Space Day, we celebrate the extra...      Friday   2019-05-03  \n",
       "10  Great to be back in Kentucky with @SecretaryAc...      Friday   2019-05-03  \n",
       "11  Proud of Law Enforcement, First Responders, &a...      Friday   2019-05-03  \n",
       "12  Honored to be with these congregations. @POTUS...      Friday   2019-05-03  \n",
       "13  Joined the pastors of  Mt Pleasant Baptist Chu...      Friday   2019-05-03  \n",
       "14  Just landed in Louisiana. Thank you for the wa...      Friday   2019-05-03  \n",
       "15  In early 2017, a top economic adviser for the ...      Friday   2019-05-03  \n",
       "16  263,000 jobs in April &amp; the LOWEST unemplo...      Friday   2019-05-03  \n",
       "17  .@POTUS has said from the beginning: if you cu...      Friday   2019-05-03  \n",
       "18  Heading to Opelousas, Louisiana to visit Mount...      Friday   2019-05-03  \n",
       "19  April marks the LOWEST unemployment in nearly ...      Friday   2019-05-03  \n",
       "20  RT @WhiteHouse: Happy Jobs Day. Tired of winni...      Friday   2019-05-03  \n",
       "21  RT @WhiteHouse: It's another historic Jobs Day...      Friday   2019-05-03  \n",
       "22  RT @realDonaldTrump: Proclamation on Days of R...    Thursday   2019-05-02  \n",
       "23  In February, @SecondLady &amp; I traveled to P...    Thursday   2019-05-02  \n",
       "24  With the strong support of Republicans in the ...    Thursday   2019-05-02  \n",
       "25  On behalf of our family, thank you for your pr...    Thursday   2019-05-02  \n",
       "26  .@SecondLady &amp; I were honored to celebrate...    Thursday   2019-05-02  \n",
       "27  RT @CCSSO: You are an amazing group of indivi...    Thursday   2019-05-02  \n",
       "28  RT @SecondLady: Great to be around so much exc...    Thursday   2019-05-02  \n",
       "29  We do well to go to the Lord in prayer. Its e...    Thursday   2019-05-02  \n",
       "30  RT @WhiteHouse: LIVE at 11 a.m. ET, President ...    Thursday   2019-05-02  \n",
       "31  As Venezuelans take to the streets to stand fo...    Thursday   2019-05-02  \n",
       "32  RT @WhiteHouse: \"So tonight we praise God for ...   Wednesday   2019-05-01  \n",
       "33  RT @WhiteHouse: \"During this holy season when ...   Wednesday   2019-05-01  \n",
       "34  RT @WhiteHouse: \"Tonight we break bread togeth...   Wednesday   2019-05-01  \n",
       "35  RT @SecondLady: Press Release: Acting Secretar...   Wednesday   2019-05-01  \n",
       "36  THANK YOU to the brave men and women of @ICEGo...   Wednesday   2019-05-01  \n",
       "37  Honored to present the Aguila award to the hea...   Wednesday   2019-05-01  \n",
       "38  RT @USCG: Coast Guard spouses will now have ac...   Wednesday   2019-05-01  \n",
       "39  Heading to Baltimore to recognize the great me...   Wednesday   2019-05-01  \n",
       "40  RT @SecondLady: HAPPENING SOON: Making announc...   Wednesday   2019-05-01  \n",
       "41  .@ActingSecDef: Its Time to Create an Americ...   Wednesday   2019-05-01  \n",
       "42  RT @realDonaldTrump: I am overriding the Decom...   Wednesday   2019-05-01  \n",
       "43  RT @realDonaldTrump: ....embargo, together wit...     Tuesday   2019-04-30  \n",
       "44  RT @realDonaldTrump: If Cuban Troops and Milit...     Tuesday   2019-04-30  \n",
       "45  To all of the men &amp; women who serve on the...     Tuesday   2019-04-30  \n",
       "46  Thank you, USS Harry S. Truman! Look forward t...     Tuesday   2019-04-30  \n",
       "47  .@POTUS asked me to deliver a message today  ...     Tuesday   2019-04-30  \n",
       "48  The Truman launched over 2,000 missions throug...     Tuesday   2019-04-30  \n",
       "49  For more than 20 years, the USS Truman &amp; i...     Tuesday   2019-04-30  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/tweets/@vp.csv\").head(50)\n",
    "\n",
    "df['day_of_week'] = pd.to_datetime(df['created_at']).dt.tz_localize('UTC').dt.tz_convert('US/Eastern').dt.day_name()\n",
    "df[\"created_at\"] = pd.to_datetime(df['created_at']).dt.tz_localize('UTC').dt.tz_convert('US/Eastern')\n",
    "df[\"created_at_2\"] = pd.to_datetime(df['created_at']).dt.date\n",
    "df\n",
    "#weekdays = calendar.day_name\n",
    "#timezone = pytz.timezone(\"US/Eastern\")\n",
    "#from_date = timezone.localize(datetime.datetime.utcnow())\n",
    "#circular_weekdays = np.tile(weekdays, 2)\n",
    "#idx = np.where(circular_weekdays == from_date.strftime(\"%A\"))[0][0]\n",
    "#weekdays_left = circular_weekdays[idx:idx+n_days]\n",
    "    \n",
    "#df = df[df[\"day_of_week\"].isin(weekdays_left)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     2019-05-06\n",
       "1     2019-05-06\n",
       "2     2019-05-05\n",
       "3     2019-05-05\n",
       "4     2019-05-05\n",
       "5     2019-05-04\n",
       "6     2019-05-04\n",
       "7     2019-05-03\n",
       "8     2019-05-03\n",
       "9     2019-05-03\n",
       "10    2019-05-03\n",
       "11    2019-05-03\n",
       "12    2019-05-03\n",
       "13    2019-05-03\n",
       "14    2019-05-03\n",
       "15    2019-05-03\n",
       "16    2019-05-03\n",
       "17    2019-05-03\n",
       "18    2019-05-03\n",
       "19    2019-05-03\n",
       "20    2019-05-03\n",
       "21    2019-05-03\n",
       "22    2019-05-02\n",
       "23    2019-05-02\n",
       "24    2019-05-02\n",
       "25    2019-05-02\n",
       "26    2019-05-02\n",
       "27    2019-05-02\n",
       "28    2019-05-02\n",
       "29    2019-05-02\n",
       "30    2019-05-02\n",
       "31    2019-05-02\n",
       "32    2019-05-01\n",
       "33    2019-05-01\n",
       "34    2019-05-01\n",
       "35    2019-05-01\n",
       "36    2019-05-01\n",
       "37    2019-05-01\n",
       "38    2019-05-01\n",
       "39    2019-05-01\n",
       "40    2019-05-01\n",
       "41    2019-05-01\n",
       "42    2019-05-01\n",
       "43    2019-04-30\n",
       "44    2019-04-30\n",
       "45    2019-04-30\n",
       "46    2019-04-30\n",
       "47    2019-04-30\n",
       "48    2019-04-30\n",
       "49    2019-04-30\n",
       "Name: created_at, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             x    y    z\n",
       "timestamp  2.0  4.0  6.0"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series({\"x\": 2, \"y\": 4, \"z\": 6 })\n",
    "s.name = \"timestamp\"\n",
    "df = s.to_frame().transpose()\n",
    "pd.DataFrame().append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Index(...) must be called with a collection of some kind, 'timestamp' was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-4e075c90b367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   7319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7320\u001b[0m     \u001b[0;31m# from BlockManager perspective\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7321\u001b[0;31m     \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_ensure_index\u001b[0;34m(index_like, copy)\u001b[0m\n\u001b[1;32m   4955\u001b[0m             \u001b[0mindex_like\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4957\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, dtype, copy, name, fastpath, tupleize_cols, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m                          **kwargs)\n\u001b[1;32m    424\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scalar_data_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtupleize_cols\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_scalar_data_error\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    881\u001b[0m         raise TypeError('{0}(...) must be called with a collection of some '\n\u001b[1;32m    882\u001b[0m                         'kind, {1} was passed'.format(cls.__name__,\n\u001b[0;32m--> 883\u001b[0;31m                                                       repr(data)))\n\u001b[0m\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Index(...) must be called with a collection of some kind, 'timestamp' was passed"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(index=\"timestamp\")\n",
    "_df = pd.Series([2,4,6]).to_frame().transpose()\n",
    "pd.concat([_df,df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
